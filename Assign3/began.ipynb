{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tensor_imshow(tensor):\n",
    "    npimg = tensor.numpy()\n",
    "    npimg = npimg.transpose(1, 2, 0)\n",
    "    print(npimg.shape)\n",
    "    np_imshow(npimg)\n",
    "\n",
    "def np_imshow(npimg):\n",
    "    plt.figure(1)\n",
    "    plt.imshow(npimg)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root='pkmn',\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Scale(64),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True, num_workers=int(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(32, 48, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv4 = nn.Conv2d(48, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=0, padding=0, ceil_mode=True)\n",
    "\n",
    "        # Starting image is of size 64, 4 times maxpooled down to 8.\n",
    "        # Map to the embedding size.\n",
    "        self.fc_to_embedding = nn.Linear(64 * 8 * 8, 128)\n",
    "        \n",
    "        # Map the embedding to an 8x8 image with 16 channels\n",
    "        self.fc_to_image = nn.Linear(128, 8 * 8 * 16)\n",
    "\n",
    "        self.conv = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv_f = nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.upsample = nn.UpsamplingNearest2d(scale_factor=2)\n",
    "\n",
    "        # Activation function\n",
    "        self.activation = F.relu\n",
    "        self.activation = lambda x: x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3, 64, 64)\n",
    "        x = self.activation(self.maxpool(self.conv1(x)))\n",
    "        x = self.activation(self.maxpool(self.conv2(x)))\n",
    "        x = self.activation(self.maxpool(self.conv3(x)))\n",
    "        x = self.activation(self.conv4(x))\n",
    "\n",
    "        # Flatten the image\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "\n",
    "        x = self.activation(self.fc_to_embedding(x))\n",
    "        \n",
    "        x = self.activation(self.fc_to_image(x))\n",
    "        # Convert to an image\n",
    "        x = x.view(-1, 16, 8, 8)\n",
    "        \n",
    "        # Apply convolutions and upsample the image\n",
    "        x = self.activation(self.upsample(self.conv(self.conv(x))))\n",
    "        x = self.activation(self.upsample(self.conv(self.conv(x))))\n",
    "        x = self.activation(self.upsample(self.conv(self.conv(x))))\n",
    "        \n",
    "        x = self.activation(self.conv_f(x))\n",
    "        \n",
    "        # Reconvert to 1D\n",
    "        x = x.view(-1, 3 * 64 * 64)\n",
    "\n",
    "        return x\n",
    "    \n",
    "D_ = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Map the embedding to an 8x8 image with 16 channels\n",
    "        self.fc1 = nn.Linear(128, 8 * 8 * 16)\n",
    "\n",
    "        self.conv = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv_f = nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.upsample = nn.UpsamplingNearest2d(scale_factor=2)\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = F.relu\n",
    "        self.activation = lambda x: x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        # Convert to an image\n",
    "        x = x.view(-1, 16, 8, 8)\n",
    "        \n",
    "        # Apply convolutions and upsample the image\n",
    "        x = self.activation(self.upsample(self.conv(self.conv(x))))\n",
    "        x = self.activation(self.upsample(self.conv(self.conv(x))))\n",
    "        x = self.activation(self.upsample(self.conv(self.conv(x))))\n",
    "        \n",
    "        x = self.activation(self.conv_f(x))\n",
    "        \n",
    "        # Reconvert to 1D\n",
    "        x = x.view(-1, 3 * 64 * 64)\n",
    "        return x\n",
    "    \n",
    "G = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator (\n",
      "  (fc1): Linear (128 -> 1024)\n",
      "  (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv_f): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (upsample): UpsamplingNearest2d(scale_factor=2)\n",
      ")\n",
      "Discriminator (\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv3): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv4): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (maxpool): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (fc_to_embedding): Linear (4096 -> 128)\n",
      "  (fc_to_image): Linear (128 -> 1024)\n",
      "  (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv_f): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (upsample): UpsamplingNearest2d(scale_factor=2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(G)\n",
    "print(D_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# D is an autoencoder, approximating Gaussian\n",
    "def D(X):\n",
    "    X_recon = D_(X)\n",
    "    # Use Laplace MLE as in the paper\n",
    "    return torch.mean(torch.sum(torch.abs(X - X_recon), 1))\n",
    "\n",
    "def reset_grad():\n",
    "    G.zero_grad()\n",
    "    D_.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "embedding_size = 128\n",
    "height = 64\n",
    "width = 64\n",
    "channels = 3\n",
    "learning_rate = 0.0002\n",
    "\n",
    "cnt = 0\n",
    "d_step = 3\n",
    "m = 5\n",
    "lam = 1e-3\n",
    "k = 0\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "G_solver = optim.Adam(G.parameters(), lr=learning_rate)\n",
    "D_solver = optim.Adam(D_.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; Convergence measure: 1.367e+04\n",
      "Iter-2; Convergence measure: 1.343e+04\n",
      "Iter-4; Convergence measure: 1.351e+04\n",
      "Iter-6; Convergence measure: 1.276e+04\n",
      "Iter-8; Convergence measure: 1.347e+04\n",
      "Iter-10; Convergence measure: 1.304e+04\n",
      "Iter-12; Convergence measure: 1.385e+04\n",
      "Iter-14; Convergence measure: 1.385e+04\n",
      "Iter-16; Convergence measure: 1.336e+04\n",
      "Iter-18; Convergence measure: 1.438e+04\n",
      "Iter-20; Convergence measure: 1.392e+04\n",
      "Iter-22; Convergence measure: 1.356e+04\n",
      "Iter-24; Convergence measure: 1.303e+04\n",
      "Iter-26; Convergence measure: 1.35e+04\n",
      "Iter-28; Convergence measure: 1.356e+04\n",
      "Iter-30; Convergence measure: 1.353e+04\n",
      "Iter-32; Convergence measure: 1.346e+04\n",
      "Iter-34; Convergence measure: 1.335e+04\n",
      "Iter-36; Convergence measure: 1.315e+04\n",
      "Iter-38; Convergence measure: 1.411e+04\n",
      "Iter-40; Convergence measure: 1.372e+04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-91:\n",
      "Process Process-90:\n",
      "Process Process-92:\n",
      "Process Process-89:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "    self.run()\n",
      "    self.run()\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n",
      "  File \"/usr/local/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n",
      "  File \"/usr/local/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n",
      "  File \"/usr/local/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "    r = index_queue.get()\n",
      "    r = index_queue.get()\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "  File \"/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/queues.py\", line 378, in get\n",
      "    racquire()\n",
      "    racquire()\n",
      "    racquire()\n",
      "    return recv()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python2.7/site-packages/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "    buf = self.recv_bytes()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-aec2843c0918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mG_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_G\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mG_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mG_solver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mreset_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    144\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torch/nn/_functions/conv.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_view4d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         grad_input = (self._grad_input(input, weight, grad_output)\n\u001b[0;32m---> 48\u001b[0;31m                       if self.needs_input_grad[0] else None)\n\u001b[0m\u001b[1;32m     49\u001b[0m         grad_weight, grad_bias = (\n\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torch/nn/_functions/conv.pyc\u001b[0m in \u001b[0;36m_grad_input\u001b[0;34m(self, input, weight, grad_output)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grad_input'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_grad_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torch/nn/_functions/conv.pyc\u001b[0m in \u001b[0;36m_thnn\u001b[0;34m(self, fn_name, input, weight, *args)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_thnn_convs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn_class_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bufs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/torch/nn/_functions/conv.pyc\u001b[0m in \u001b[0;36mcall_grad_input\u001b[0;34m(self, bufs, input, weight, grad_output)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         getattr(backend, fn.name)(backend.library_state, input, grad_output,\n\u001b[0;32m--> 246\u001b[0;31m                                   grad_input, weight, *args)\n\u001b[0m\u001b[1;32m    247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_grad_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train_iter = 2\n",
    "for batch_idx, (data, target) in enumerate(dataloader):\n",
    "    # Sample data\n",
    "    data = data.view(-1, height * width * channels)\n",
    "    X = Variable(data)\n",
    "\n",
    "    # Dicriminator\n",
    "    # Create a random tensor of size [mini_batch x embedding_vector_dim]\n",
    "    z_D = Variable(torch.randn(batch_size, embedding_size))\n",
    "\n",
    "    # Define discriminator loss\n",
    "    D_loss = D(X) - k * D(G(z_D))\n",
    "\n",
    "    D_loss.backward()\n",
    "    D_solver.step()\n",
    "    reset_grad()\n",
    "\n",
    "    # Generator\n",
    "    z_G = Variable(torch.randn(batch_size, embedding_size))\n",
    "\n",
    "    G_loss = D(G(z_G))\n",
    "\n",
    "    G_loss.backward()\n",
    "    G_solver.step()\n",
    "    reset_grad()\n",
    "\n",
    "    # Update k, the equlibrium\n",
    "    k = k + lam * (gamma*D(X) - D(G(z_G)))\n",
    "    k = k.data[0]\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if batch_idx % 100 == 0:\n",
    "        measure = D(X) + torch.abs(gamma*D(X) - D(G(z_G)))\n",
    "\n",
    "        print('Iter-{}; Convergence measure: {:.4}'\n",
    "              .format(batch_idx, measure.data[0]))\n",
    "\n",
    "        samples = G(z_G).data.numpy()[:16]\n",
    "\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(3, 64, 64).transpose(1, 2, 0))\n",
    "\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "\n",
    "        plt.savefig('out/{}.png'.format(str(cnt).zfill(3)), bbox_inches='tight')\n",
    "        cnt += 1\n",
    "        plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
