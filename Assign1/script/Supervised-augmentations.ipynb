{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "import pickle \n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded\n",
      "3000\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "trainset_imoprt = pickle.load(open(\"../data/kaggle/train_labeled.p\", \"rb\"))\n",
    "validset_import = pickle.load(open(\"../data/kaggle/validation.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset_imoprt, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(validset_import, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d(0.3)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# optim.SGD(params, lr=<object>, momentum=0, dampening=0, weight_decay=0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.8)\n",
    "\n",
    "# optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Single channel only\n",
    "def tensor_imshow(tensor):    \n",
    "    npimg = tensor.numpy()[0]\n",
    "    np_imshow(npimg)\n",
    "    \n",
    "def np_imshow(npimg):\n",
    "    plt.figure(1)\n",
    "    plt.imshow(npimg, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clipped_zoom(img, zoom_factor, **kwargs):\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # width and height of the zoomed image\n",
    "    zh = int(np.round(zoom_factor * h))\n",
    "    zw = int(np.round(zoom_factor * w))\n",
    "\n",
    "    # for multichannel images we don't want to apply the zoom factor to the RGB\n",
    "    # dimension, so instead we create a tuple of zoom factors, one per array\n",
    "    # dimension, with 1's for any trailing dimensions after the width and height.\n",
    "    zoom_tuple = (zoom_factor,) * 2 + (1,) * (img.ndim - 2)\n",
    "\n",
    "    # zooming out\n",
    "    if zoom_factor < 1:\n",
    "        # bounding box of the clip region within the output array\n",
    "        top = (h - zh) // 2\n",
    "        left = (w - zw) // 2\n",
    "        # zero-padding\n",
    "        out = np.zeros_like(img)\n",
    "        out += np.amin(img)\n",
    "        out[top:top+zh, left:left+zw] = ndimage.zoom(img, zoom_tuple, **kwargs)\n",
    "\n",
    "    # zooming in\n",
    "    elif zoom_factor > 1:\n",
    "        # bounding box of the clip region within the input array\n",
    "        top = (zh - h) // 2\n",
    "        left = (zw - w) // 2\n",
    "        out = ndimage.zoom(img[top:top+zh, left:left+zw], zoom_tuple, **kwargs)\n",
    "        # `out` might still be slightly larger than `img` due to rounding, so\n",
    "        # trim off any extra pixels at the edges\n",
    "        trim_top = ((out.shape[0] - h) // 2)\n",
    "        trim_left = ((out.shape[1] - w) // 2)\n",
    "        out = out[trim_top:trim_top+h, trim_left:trim_left+w]\n",
    "\n",
    "    # if zoom_factor == 1, just return the input array\n",
    "    else:\n",
    "        out = img\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Augmentations\n",
    "\n",
    "# Transforms a tensor into a numpy array\n",
    "# ONLY FOR SINGLE CHANNEL\n",
    "def to_npimg(tensor):\n",
    "    t = (tensor[0]).numpy()\n",
    "    return t\n",
    "\n",
    "\n",
    "def random_translate(npimg, min_translate=-0.2, max_translate=0.2):\n",
    "    \n",
    "    h, w = npimg.shape\n",
    "    min_pixels = ((h + w) / 2) * min_translate\n",
    "    max_pixels = ((h + w) / 2) * max_translate\n",
    "    shift = int(random.uniform(min_pixels, max_pixels))\n",
    "    \n",
    "    return ndimage.interpolation.shift(npimg, shift, mode='nearest')\n",
    "\n",
    "\n",
    "def random_rotate(npimg, min_rotate=-30, max_rotate=30):\n",
    "    \n",
    "    theta = random.randint(int(min_rotate), int(max_rotate + 1))\n",
    "    return ndimage.interpolation.rotate(npimg, theta, reshape=False, mode='nearest')\n",
    "\n",
    "\n",
    "def random_scale(npimg, min_scale=0.5, max_scale=1.5):\n",
    "    \n",
    "    scale_factor = random.uniform(min_scale, max_scale)\n",
    "    sc_img = clipped_zoom(npimg, scale_factor, mode='nearest')\n",
    "    return sc_img\n",
    "    \n",
    "\n",
    "# Converts a HxW ndarray into a 1xHxW tensor\n",
    "def to_tensor(npimg):\n",
    "    h, w = npimg.shape\n",
    "    npimg = npimg.reshape(1, h, w)\n",
    "    return torch.from_numpy(npimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# CPU only training\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            \n",
    "            data[i] = transforms.Compose([\n",
    "                transforms.Lambda(lambda x: to_npimg(x)),\n",
    "                transforms.Lambda(lambda x: random_translate(x)),\n",
    "                transforms.Lambda(lambda x: random_rotate(x)),\n",
    "                transforms.Lambda(lambda x: random_scale(x)),\n",
    "                transforms.Lambda(lambda x: to_tensor(x)),\n",
    "                ])(data[i])\n",
    "        \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3000 (0%)]\tLoss: 2.308381\n",
      "Train Epoch: 1 [640/3000 (21%)]\tLoss: 2.313405\n",
      "Train Epoch: 1 [1280/3000 (43%)]\tLoss: 2.315204\n",
      "Train Epoch: 1 [1920/3000 (64%)]\tLoss: 2.303674\n",
      "Train Epoch: 1 [2560/3000 (85%)]\tLoss: 2.306613\n"
     ]
    }
   ],
   "source": [
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch, valid_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3000 (0%)]\tLoss: 2.299500\n",
      "Train Epoch: 1 [640/3000 (21%)]\tLoss: 2.300411\n",
      "Train Epoch: 1 [1280/3000 (43%)]\tLoss: 2.306949\n",
      "Train Epoch: 1 [1920/3000 (64%)]\tLoss: 2.291730\n",
      "Train Epoch: 1 [2560/3000 (85%)]\tLoss: 2.283439\n",
      "\n",
      "Test set: Average loss: 2.2728, Accuracy: 1985/10000 (20%)\n",
      "\n",
      "Train Epoch: 2 [0/3000 (0%)]\tLoss: 2.289344\n",
      "Train Epoch: 2 [640/3000 (21%)]\tLoss: 2.268068\n",
      "Train Epoch: 2 [1280/3000 (43%)]\tLoss: 2.259471\n",
      "Train Epoch: 2 [1920/3000 (64%)]\tLoss: 2.265214\n",
      "Train Epoch: 2 [2560/3000 (85%)]\tLoss: 2.269572\n",
      "\n",
      "Test set: Average loss: 2.2069, Accuracy: 2694/10000 (27%)\n",
      "\n",
      "Train Epoch: 3 [0/3000 (0%)]\tLoss: 2.282565\n",
      "Train Epoch: 3 [640/3000 (21%)]\tLoss: 2.281619\n",
      "Train Epoch: 3 [1280/3000 (43%)]\tLoss: 2.278317\n",
      "Train Epoch: 3 [1920/3000 (64%)]\tLoss: 2.248786\n",
      "Train Epoch: 3 [2560/3000 (85%)]\tLoss: 2.219997\n",
      "\n",
      "Test set: Average loss: 2.0750, Accuracy: 3481/10000 (35%)\n",
      "\n",
      "Train Epoch: 4 [0/3000 (0%)]\tLoss: 2.213680\n",
      "Train Epoch: 4 [640/3000 (21%)]\tLoss: 2.213913\n",
      "Train Epoch: 4 [1280/3000 (43%)]\tLoss: 2.178870\n",
      "Train Epoch: 4 [1920/3000 (64%)]\tLoss: 2.133575\n",
      "Train Epoch: 4 [2560/3000 (85%)]\tLoss: 2.191436\n",
      "\n",
      "Test set: Average loss: 1.8639, Accuracy: 4856/10000 (49%)\n",
      "\n",
      "Train Epoch: 5 [0/3000 (0%)]\tLoss: 1.954210\n",
      "Train Epoch: 5 [640/3000 (21%)]\tLoss: 2.201651\n",
      "Train Epoch: 5 [1280/3000 (43%)]\tLoss: 2.146705\n",
      "Train Epoch: 5 [1920/3000 (64%)]\tLoss: 2.023336\n",
      "Train Epoch: 5 [2560/3000 (85%)]\tLoss: 2.077585\n",
      "\n",
      "Test set: Average loss: 1.6714, Accuracy: 5442/10000 (54%)\n",
      "\n",
      "Train Epoch: 6 [0/3000 (0%)]\tLoss: 2.111943\n",
      "Train Epoch: 6 [640/3000 (21%)]\tLoss: 2.049271\n",
      "Train Epoch: 6 [1280/3000 (43%)]\tLoss: 1.950161\n",
      "Train Epoch: 6 [1920/3000 (64%)]\tLoss: 1.968399\n",
      "Train Epoch: 6 [2560/3000 (85%)]\tLoss: 2.112033\n",
      "\n",
      "Test set: Average loss: 1.5520, Accuracy: 6013/10000 (60%)\n",
      "\n",
      "Train Epoch: 7 [0/3000 (0%)]\tLoss: 2.047924\n",
      "Train Epoch: 7 [640/3000 (21%)]\tLoss: 1.921148\n",
      "Train Epoch: 7 [1280/3000 (43%)]\tLoss: 1.983486\n",
      "Train Epoch: 7 [1920/3000 (64%)]\tLoss: 2.071594\n",
      "Train Epoch: 7 [2560/3000 (85%)]\tLoss: 1.910404\n",
      "\n",
      "Test set: Average loss: 1.4325, Accuracy: 6359/10000 (64%)\n",
      "\n",
      "Train Epoch: 8 [0/3000 (0%)]\tLoss: 1.811045\n",
      "Train Epoch: 8 [640/3000 (21%)]\tLoss: 1.902143\n",
      "Train Epoch: 8 [1280/3000 (43%)]\tLoss: 1.965064\n",
      "Train Epoch: 8 [1920/3000 (64%)]\tLoss: 1.895069\n",
      "Train Epoch: 8 [2560/3000 (85%)]\tLoss: 1.960972\n",
      "\n",
      "Test set: Average loss: 1.3457, Accuracy: 6560/10000 (66%)\n",
      "\n",
      "Train Epoch: 9 [0/3000 (0%)]\tLoss: 1.875618\n",
      "Train Epoch: 9 [640/3000 (21%)]\tLoss: 1.666555\n",
      "Train Epoch: 9 [1280/3000 (43%)]\tLoss: 2.040684\n",
      "Train Epoch: 9 [1920/3000 (64%)]\tLoss: 1.781499\n",
      "Train Epoch: 9 [2560/3000 (85%)]\tLoss: 2.096443\n",
      "\n",
      "Test set: Average loss: 1.1738, Accuracy: 7374/10000 (74%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create Sample Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "testset = pickle.load(open(\"../data/kaggle/test.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(testset,batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Test Accuuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1179, Accuracy: 9635/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "label_predict = np.array([])\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = model(data)\n",
    "    temp = output.data.max(1)[1].numpy().reshape(-1)\n",
    "    label_predict = np.concatenate((label_predict, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.,  2.,  1., ...,  4.,  5.,  6.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "predict_label = pd.DataFrame(label_predict, columns=['label'], dtype=int)\n",
    "predict_label.reset_index(inplace=True)\n",
    "predict_label.rename(columns={'index': 'ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  label\n",
       "0   0      7\n",
       "1   1      2\n",
       "2   2      1\n",
       "3   3      0\n",
       "4   4      4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predict_label.to_csv('../data/kaggle/sample_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
