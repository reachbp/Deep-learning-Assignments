{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle \n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(trainset_new, batch_size=64, shuffle=True)\n",
    "# valid_loader = torch.utils.data.DataLoader(validset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded\n",
      "3000\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "trainset_imoprt = pickle.load(open(\"../data/kaggle/train_labeled.p\", \"rb\"))\n",
    "validset_import = pickle.load(open(\"../data/kaggle/validation.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset_imoprt, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(validset_import, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if False:\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        \n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d(0.3)\n",
    "        self.fc1 = nn.Linear(320, 160)\n",
    "        self.fc2 = nn.Linear(160, 10)\n",
    "        self.fc3 = nn.Linear(80, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.data.size())\n",
    "        x = x.view(-1, 1, 28,28)\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return F.log_softmax(x)\n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reconstruction_function = nn.BCELoss()\n",
    "reconstruction_function.size_average = False\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = reconstruction_function(recon_x, x)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CPU only training\n",
    "def generate_train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0] / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "    \n",
    "def generate_test(epoch, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, _ in test_loader:\n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += loss_function(recon_batch, data, mu, logvar).data[0]\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3000 (0%)]\tLoss: 549.031494\n",
      "====> Epoch: 1 Average loss: -3437.4776\n",
      "====> Test set loss: -5281.2791\n",
      "Train Epoch: 2 [0/3000 (0%)]\tLoss: -5263.533203\n",
      "====> Epoch: 2 Average loss: -5903.2450\n",
      "====> Test set loss: -6628.2332\n",
      "Train Epoch: 3 [0/3000 (0%)]\tLoss: -6614.455078\n",
      "====> Epoch: 3 Average loss: -7122.7477\n",
      "====> Test set loss: -7527.5857\n",
      "Train Epoch: 4 [0/3000 (0%)]\tLoss: -7336.771484\n",
      "====> Epoch: 4 Average loss: -7779.5994\n",
      "====> Test set loss: -7961.1549\n",
      "Train Epoch: 5 [0/3000 (0%)]\tLoss: -8257.661133\n",
      "====> Epoch: 5 Average loss: -8110.3779\n",
      "====> Test set loss: -8196.7052\n",
      "Train Epoch: 6 [0/3000 (0%)]\tLoss: -8029.854980\n",
      "====> Epoch: 6 Average loss: -8335.8792\n",
      "====> Test set loss: -8407.7363\n",
      "Train Epoch: 7 [0/3000 (0%)]\tLoss: -8241.693359\n",
      "====> Epoch: 7 Average loss: -8523.5153\n",
      "====> Test set loss: -8557.3710\n",
      "Train Epoch: 8 [0/3000 (0%)]\tLoss: -8521.340820\n",
      "====> Epoch: 8 Average loss: -8699.2746\n",
      "====> Test set loss: -8762.1284\n",
      "Train Epoch: 9 [0/3000 (0%)]\tLoss: -8958.595703\n",
      "====> Epoch: 9 Average loss: -8855.1176\n",
      "====> Test set loss: -8867.9603\n",
      "Train Epoch: 10 [0/3000 (0%)]\tLoss: -8918.667969\n",
      "====> Epoch: 10 Average loss: -8978.5808\n",
      "====> Test set loss: -8985.8964\n",
      "Train Epoch: 11 [0/3000 (0%)]\tLoss: -9049.470703\n",
      "====> Epoch: 11 Average loss: -9079.5403\n",
      "====> Test set loss: -9085.2361\n",
      "Train Epoch: 12 [0/3000 (0%)]\tLoss: -9061.045898\n",
      "====> Epoch: 12 Average loss: -9179.4345\n",
      "====> Test set loss: -9172.2521\n",
      "Train Epoch: 13 [0/3000 (0%)]\tLoss: -9187.661133\n",
      "====> Epoch: 13 Average loss: -9274.8519\n",
      "====> Test set loss: -9270.8126\n",
      "Train Epoch: 14 [0/3000 (0%)]\tLoss: -9145.832031\n",
      "====> Epoch: 14 Average loss: -9365.5278\n",
      "====> Test set loss: -9342.5796\n",
      "Train Epoch: 15 [0/3000 (0%)]\tLoss: -9591.351562\n",
      "====> Epoch: 15 Average loss: -9450.8903\n",
      "====> Test set loss: -9434.1490\n",
      "Train Epoch: 16 [0/3000 (0%)]\tLoss: -9790.646484\n",
      "====> Epoch: 16 Average loss: -9528.0485\n",
      "====> Test set loss: -9485.2286\n",
      "Train Epoch: 17 [0/3000 (0%)]\tLoss: -9792.833008\n",
      "====> Epoch: 17 Average loss: -9593.3937\n",
      "====> Test set loss: -9550.7705\n",
      "Train Epoch: 18 [0/3000 (0%)]\tLoss: -9930.932617\n",
      "====> Epoch: 18 Average loss: -9659.4007\n",
      "====> Test set loss: -9593.0749\n",
      "Train Epoch: 19 [0/3000 (0%)]\tLoss: -9336.297852\n",
      "====> Epoch: 19 Average loss: -9712.8129\n",
      "====> Test set loss: -9657.7483\n",
      "Train Epoch: 20 [0/3000 (0%)]\tLoss: -9461.629883\n",
      "====> Epoch: 20 Average loss: -9768.6795\n",
      "====> Test set loss: -9687.6732\n",
      "Train Epoch: 21 [0/3000 (0%)]\tLoss: -9841.657227\n",
      "====> Epoch: 21 Average loss: -9811.3678\n",
      "====> Test set loss: -9744.1530\n",
      "Train Epoch: 22 [0/3000 (0%)]\tLoss: -10057.067383\n",
      "====> Epoch: 22 Average loss: -9855.2033\n",
      "====> Test set loss: -9751.8074\n",
      "Train Epoch: 23 [0/3000 (0%)]\tLoss: -9578.460938\n",
      "====> Epoch: 23 Average loss: -9896.8796\n",
      "====> Test set loss: -9798.8198\n",
      "Train Epoch: 24 [0/3000 (0%)]\tLoss: -9684.719727\n",
      "====> Epoch: 24 Average loss: -9936.8405\n",
      "====> Test set loss: -9847.3467\n",
      "Train Epoch: 25 [0/3000 (0%)]\tLoss: -10022.255859\n",
      "====> Epoch: 25 Average loss: -9975.1996\n",
      "====> Test set loss: -9876.3125\n",
      "Train Epoch: 26 [0/3000 (0%)]\tLoss: -10071.890625\n",
      "====> Epoch: 26 Average loss: -10008.2693\n",
      "====> Test set loss: -9910.1772\n",
      "Train Epoch: 27 [0/3000 (0%)]\tLoss: -10240.753906\n",
      "====> Epoch: 27 Average loss: -10037.2294\n",
      "====> Test set loss: -9919.6605\n",
      "Train Epoch: 28 [0/3000 (0%)]\tLoss: -9830.403320\n",
      "====> Epoch: 28 Average loss: -10070.9018\n",
      "====> Test set loss: -9952.9336\n",
      "Train Epoch: 29 [0/3000 (0%)]\tLoss: -9879.548828\n",
      "====> Epoch: 29 Average loss: -10098.0720\n",
      "====> Test set loss: -9949.5633\n",
      "Train Epoch: 30 [0/3000 (0%)]\tLoss: -10168.596680\n",
      "====> Epoch: 30 Average loss: -10124.7940\n",
      "====> Test set loss: -9986.0314\n",
      "Train Epoch: 31 [0/3000 (0%)]\tLoss: -10404.361328\n",
      "====> Epoch: 31 Average loss: -10149.5100\n",
      "====> Test set loss: -9985.8824\n",
      "Train Epoch: 32 [0/3000 (0%)]\tLoss: -9789.818359\n",
      "====> Epoch: 32 Average loss: -10174.2672\n",
      "====> Test set loss: -10023.7720\n",
      "Train Epoch: 33 [0/3000 (0%)]\tLoss: -10052.970703\n",
      "====> Epoch: 33 Average loss: -10196.8341\n",
      "====> Test set loss: -10047.6109\n",
      "Train Epoch: 34 [0/3000 (0%)]\tLoss: -9959.633789\n",
      "====> Epoch: 34 Average loss: -10219.9738\n",
      "====> Test set loss: -10070.5793\n",
      "Train Epoch: 35 [0/3000 (0%)]\tLoss: -10386.398438\n",
      "====> Epoch: 35 Average loss: -10241.7790\n",
      "====> Test set loss: -10083.2374\n",
      "Train Epoch: 36 [0/3000 (0%)]\tLoss: -10239.652344\n",
      "====> Epoch: 36 Average loss: -10265.4611\n",
      "====> Test set loss: -10089.3803\n",
      "Train Epoch: 37 [0/3000 (0%)]\tLoss: -10453.358398\n",
      "====> Epoch: 37 Average loss: -10284.0248\n",
      "====> Test set loss: -10119.6165\n",
      "Train Epoch: 38 [0/3000 (0%)]\tLoss: -10364.573242\n",
      "====> Epoch: 38 Average loss: -10308.7928\n",
      "====> Test set loss: -10154.0385\n",
      "Train Epoch: 39 [0/3000 (0%)]\tLoss: -10425.324219\n",
      "====> Epoch: 39 Average loss: -10330.2130\n",
      "====> Test set loss: -10167.4372\n",
      "Train Epoch: 40 [0/3000 (0%)]\tLoss: -10309.381836\n",
      "====> Epoch: 40 Average loss: -10345.0391\n",
      "====> Test set loss: -10177.4375\n",
      "Train Epoch: 41 [0/3000 (0%)]\tLoss: -10604.894531\n",
      "====> Epoch: 41 Average loss: -10366.5733\n",
      "====> Test set loss: -10185.3840\n",
      "Train Epoch: 42 [0/3000 (0%)]\tLoss: -10344.558594\n",
      "====> Epoch: 42 Average loss: -10380.9028\n",
      "====> Test set loss: -10186.6936\n",
      "Train Epoch: 43 [0/3000 (0%)]\tLoss: -10290.141602\n",
      "====> Epoch: 43 Average loss: -10398.6686\n",
      "====> Test set loss: -10210.5597\n",
      "Train Epoch: 44 [0/3000 (0%)]\tLoss: -10561.311523\n",
      "====> Epoch: 44 Average loss: -10415.4273\n",
      "====> Test set loss: -10230.2613\n",
      "Train Epoch: 45 [0/3000 (0%)]\tLoss: -10146.070312\n",
      "====> Epoch: 45 Average loss: -10426.4306\n",
      "====> Test set loss: -10226.9844\n",
      "Train Epoch: 46 [0/3000 (0%)]\tLoss: -10620.208008\n",
      "====> Epoch: 46 Average loss: -10444.7685\n",
      "====> Test set loss: -10260.0161\n",
      "Train Epoch: 47 [0/3000 (0%)]\tLoss: -10116.903320\n",
      "====> Epoch: 47 Average loss: -10457.9735\n",
      "====> Test set loss: -10265.4717\n",
      "Train Epoch: 48 [0/3000 (0%)]\tLoss: -10329.957031\n",
      "====> Epoch: 48 Average loss: -10470.3468\n",
      "====> Test set loss: -10268.0427\n",
      "Train Epoch: 49 [0/3000 (0%)]\tLoss: -10684.290039\n",
      "====> Epoch: 49 Average loss: -10483.0119\n",
      "====> Test set loss: -10284.7812\n"
     ]
    }
   ],
   "source": [
    "#Generative phase\n",
    "for epoch in range(1, 50):\n",
    "    generate_train(epoch)\n",
    "    generate_test(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check generated images\n",
    "from  torchvision.utils import save_image\n",
    "for index, (data, target) in enumerate(train_loader):\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    gen_data, mu, logvar = model(data)\n",
    "    gen_images = gen_data.view(-1, 1,28,28)\n",
    "    #print(data[0])\n",
    "    save_image(gen_images.data, 'images/gen{}.jpg'.format(index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CPU only training\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.8)\n",
    "def train(epoch):\n",
    "    classifier.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        gen_data, mu, logvar = model(data)\n",
    "        #Check reconstruction loss \n",
    "        recon_loss = reconstruction_function(gen_data, data)\n",
    "        #print(\"Reconstruction loss is\", recon_loss)\n",
    "        #print(\"Type of generated data\", type(gen_data))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = classifier(gen_data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test(epoch, valid_loader):\n",
    "    classifier.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        gen_data, mu, logvar = model(data)\n",
    "        output = classifier(gen_data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3000 (0%)]\tLoss: 2.303313\n",
      "Train Epoch: 1 [640/3000 (21%)]\tLoss: 2.299531\n",
      "Train Epoch: 1 [1280/3000 (43%)]\tLoss: 2.311447\n",
      "Train Epoch: 1 [1920/3000 (64%)]\tLoss: 2.304488\n",
      "Train Epoch: 1 [2560/3000 (85%)]\tLoss: 2.315426\n",
      "\n",
      "Test set: Average loss: 2.3043, Accuracy: 949/10000 (9%)\n",
      "\n",
      "Train Epoch: 2 [0/3000 (0%)]\tLoss: 2.296243\n",
      "Train Epoch: 2 [640/3000 (21%)]\tLoss: 2.302421\n",
      "Train Epoch: 2 [1280/3000 (43%)]\tLoss: 2.302147\n",
      "Train Epoch: 2 [1920/3000 (64%)]\tLoss: 2.301919\n",
      "Train Epoch: 2 [2560/3000 (85%)]\tLoss: 2.304081\n",
      "\n",
      "Test set: Average loss: 2.3043, Accuracy: 948/10000 (9%)\n",
      "\n",
      "Train Epoch: 3 [0/3000 (0%)]\tLoss: 2.303864\n",
      "Train Epoch: 3 [640/3000 (21%)]\tLoss: 2.303399\n",
      "Train Epoch: 3 [1280/3000 (43%)]\tLoss: 2.308649\n",
      "Train Epoch: 3 [1920/3000 (64%)]\tLoss: 2.307373\n",
      "Train Epoch: 3 [2560/3000 (85%)]\tLoss: 2.313890\n",
      "\n",
      "Test set: Average loss: 2.3041, Accuracy: 951/10000 (10%)\n",
      "\n",
      "Train Epoch: 4 [0/3000 (0%)]\tLoss: 2.306673\n",
      "Train Epoch: 4 [640/3000 (21%)]\tLoss: 2.310776\n",
      "Train Epoch: 4 [1280/3000 (43%)]\tLoss: 2.298822\n",
      "Train Epoch: 4 [1920/3000 (64%)]\tLoss: 2.305757\n",
      "Train Epoch: 4 [2560/3000 (85%)]\tLoss: 2.310771\n",
      "\n",
      "Test set: Average loss: 2.3042, Accuracy: 955/10000 (10%)\n",
      "\n",
      "Train Epoch: 5 [0/3000 (0%)]\tLoss: 2.304310\n",
      "Train Epoch: 5 [640/3000 (21%)]\tLoss: 2.299248\n",
      "Train Epoch: 5 [1280/3000 (43%)]\tLoss: 2.301275\n",
      "Train Epoch: 5 [1920/3000 (64%)]\tLoss: 2.309110\n",
      "Train Epoch: 5 [2560/3000 (85%)]\tLoss: 2.306186\n",
      "\n",
      "Test set: Average loss: 2.3040, Accuracy: 952/10000 (10%)\n",
      "\n",
      "Train Epoch: 6 [0/3000 (0%)]\tLoss: 2.306867\n",
      "Train Epoch: 6 [640/3000 (21%)]\tLoss: 2.302677\n",
      "Train Epoch: 6 [1280/3000 (43%)]\tLoss: 2.302647\n",
      "Train Epoch: 6 [1920/3000 (64%)]\tLoss: 2.303769\n",
      "Train Epoch: 6 [2560/3000 (85%)]\tLoss: 2.302253\n",
      "\n",
      "Test set: Average loss: 2.3040, Accuracy: 952/10000 (10%)\n",
      "\n",
      "Train Epoch: 7 [0/3000 (0%)]\tLoss: 2.295228\n",
      "Train Epoch: 7 [640/3000 (21%)]\tLoss: 2.312875\n",
      "Train Epoch: 7 [1280/3000 (43%)]\tLoss: 2.297070\n",
      "Train Epoch: 7 [1920/3000 (64%)]\tLoss: 2.305536\n",
      "Train Epoch: 7 [2560/3000 (85%)]\tLoss: 2.310316\n",
      "\n",
      "Test set: Average loss: 2.3038, Accuracy: 954/10000 (10%)\n",
      "\n",
      "Train Epoch: 8 [0/3000 (0%)]\tLoss: 2.313638\n",
      "Train Epoch: 8 [640/3000 (21%)]\tLoss: 2.306834\n",
      "Train Epoch: 8 [1280/3000 (43%)]\tLoss: 2.304351\n",
      "Train Epoch: 8 [1920/3000 (64%)]\tLoss: 2.309371\n",
      "Train Epoch: 8 [2560/3000 (85%)]\tLoss: 2.302615\n",
      "\n",
      "Test set: Average loss: 2.3037, Accuracy: 951/10000 (10%)\n",
      "\n",
      "Train Epoch: 9 [0/3000 (0%)]\tLoss: 2.301514\n",
      "Train Epoch: 9 [640/3000 (21%)]\tLoss: 2.308340\n",
      "Train Epoch: 9 [1280/3000 (43%)]\tLoss: 2.313092\n",
      "Train Epoch: 9 [1920/3000 (64%)]\tLoss: 2.305234\n",
      "Train Epoch: 9 [2560/3000 (85%)]\tLoss: 2.315604\n",
      "\n",
      "Test set: Average loss: 2.3036, Accuracy: 949/10000 (9%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification phase\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sample Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testset = pickle.load(open(\"../data/kaggle/test.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(testset,batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1253, Accuracy: 9586/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_predict = np.array([])\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = model(data)\n",
    "    temp = output.data.max(1)[1].numpy().reshape(-1)\n",
    "    label_predict = np.concatenate((label_predict, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.,  2.,  1., ...,  4.,  5.,  6.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_label = pd.DataFrame(label_predict, columns=['label'], dtype=int)\n",
    "predict_label.reset_index(inplace=True)\n",
    "predict_label.rename(columns={'index': 'ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  label\n",
       "0   0      7\n",
       "1   1      2\n",
       "2   2      1\n",
       "3   3      0\n",
       "4   4      4"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_label.to_csv('../data/kaggle/sample_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
