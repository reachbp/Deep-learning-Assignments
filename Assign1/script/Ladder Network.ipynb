{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle \n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(trainset_new, batch_size=64, shuffle=True)\n",
    "# valid_loader = torch.utils.data.DataLoader(validset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainset_imoprt = pickle.load(open(\"../data/kaggle/train_labeled.p\", \"rb\"))\n",
    "validset_import = pickle.load(open(\"../data/kaggle/validation.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset_imoprt, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(validset_import, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Ladder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Ladder, self).__init__()\n",
    "        \n",
    "        self.noise_level = 0.3\n",
    "        self.layer_sizes = [1000,500,250,250,250]\n",
    "        self.layer_sizes[0] = 784\n",
    "        \n",
    "        self.clean_z = {}\n",
    "        self.clz_bn = {}\n",
    "        self.noisy_z = {}\n",
    "    \n",
    "        #self.noiser = self.addNoise()\n",
    "    def addNoise(self, x, mu = 0, std = 1):\n",
    "        print(\"size of tensor to add noise to\", x.size(1))\n",
    "        noise = torch.FloatTensor(len(x), x.size(1))\n",
    "        noiseV = Variable(noise)\n",
    "        noiseV.resize_as(x)\n",
    "        noiseV.data.normal_(mu, std)\n",
    "        x.data.add_(noiseV.data)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, input):\n",
    "        #self.input = nn.Identity()()\n",
    "        print(\"Check point 1\", input.size())\n",
    "        self.clean_z[0] = input.view(-1, self.layer_sizes[0])\n",
    "        print(\"Check point 2\", self.clean_z[0].size())\n",
    "        self.noisy_z[0] = self.addNoise(self.clean_z[0])\n",
    "        print(\"Check point 3\", self.noisy_z[0].size())\n",
    "        prev_out = self.noisy_z[0]\n",
    "        print(\"Check point 4\")\n",
    "        bn_layers = {}\n",
    "        \n",
    "        for i in range(1,len(self.layer_sizes)) :\n",
    "            sz = self.layer_sizes[i]\n",
    "            self.clean_z[i] = nn.Linear(self.layer_sizes[i-1], sz)(prev_out)\n",
    "            print(\"Check point 5\")\n",
    "            bn_layers[i] = nn.BatchNorm1d(sz, 0, 0)\n",
    "            print(\"Check point 5.1\", self.clean_z[i].size())\n",
    "            self.clz_bn[i] = bn_layers[i](self.clean_z[i])\n",
    "            print(\"Check point 6\")\n",
    "            self.noisy_z[i] = self.addNoise(self.clz_bn[i])\n",
    "            print(\"Check point 7\", self.noisy_z[i].size(),sz)\n",
    "            prev_out = F.relu(self.noisy_z[i].add(sz))\n",
    "            print(\"Check point 8\", prev_out.size())\n",
    "       \n",
    "        y = nn.Linear(250, 10)(prev_out)\n",
    "        y_bn = nn.BatchNorm1d(10)(y)\n",
    "        print(\"Output \", y_bn.size())\n",
    "        #y_softmax = nn.SoftMax()(y_bn)\n",
    "        #print(\"output size \", y_softmax.size())\n",
    "        return y_bn\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.encode(input.view(-1,784))\n",
    "        return output\n",
    "model = Ladder()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check point 1 torch.Size([64, 784])\n",
      "Check point 2 torch.Size([64, 784])\n",
      "size of tensor to add noise to 784\n",
      "Check point 3 torch.Size([64, 784])\n",
      "Check point 4\n",
      "Check point 5\n",
      "Check point 5.1 torch.Size([64, 500])\n",
      "Check point 6\n",
      "size of tensor to add noise to 500\n",
      "Check point 7 torch.Size([64, 500]) 500\n",
      "Check point 8 torch.Size([64, 500])\n",
      "Check point 5\n",
      "Check point 5.1 torch.Size([64, 250])\n",
      "Check point 6\n",
      "size of tensor to add noise to 250\n",
      "Check point 7 torch.Size([64, 250]) 250\n",
      "Check point 8 torch.Size([64, 250])\n",
      "Check point 5\n",
      "Check point 5.1 torch.Size([64, 250])\n",
      "Check point 6\n",
      "size of tensor to add noise to 250\n",
      "Check point 7 torch.Size([64, 250]) 250\n",
      "Check point 8 torch.Size([64, 250])\n",
      "Check point 5\n",
      "Check point 5.1 torch.Size([64, 250])\n",
      "Check point 6\n",
      "size of tensor to add noise to 250\n",
      "Check point 7 torch.Size([64, 250]) 250\n",
      "Check point 8 torch.Size([64, 250])\n",
      "Output  torch.Size([64, 10])\n",
      "<class 'torch.autograd.variable.Variable'>\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.Tensor(64, 1, 28,28))\n",
    "output = model(input)\n",
    "print(type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = torch.load(\"decon-vae-50.p\")\n",
    "model_name = 'denoise'\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CPU only training\n",
    "def generate_train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    classification_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        # Add noise\n",
    "        if model_name == 'denoise':\n",
    "            noise = torch.FloatTensor(len(data), 1,28,28)\n",
    "            noiseV = Variable(noise)\n",
    "            noiseV.resize_as(data)\n",
    "            noiseV.data.normal_(0, 1)\n",
    "            data.data.add_(noiseV.data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x_new, mu, logvar = model(data)\n",
    "        kl_loss = 0.5 * torch.mean(torch.exp(logvar) + mu ** 2 - 1. - logvar)\n",
    "        recon_loss = mse(x_new, data) + kl_loss\n",
    "        output = F.log_softmax(mu)\n",
    "        class_loss = F.nll_loss(output, target)\n",
    "        loss = recon_loss + class_loss\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        classification_loss += class_loss.data[0]\n",
    "        optimizer.step()\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, classification_loss / len(train_loader.dataset)))\n",
    "\n",
    "    \n",
    "def generate_test(epoch, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        x_new, mu, logvar = model(data)\n",
    "        output = F.log_softmax(mu) \n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "        \n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generative Classifier\n",
    "for epoch in range(1, 50):\n",
    "    generate_train(epoch)\n",
    "    generate_test(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model, \"decon-vae-50.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check generated images\n",
    "from  torchvision.utils import save_image\n",
    "for index, (data, target) in enumerate(train_loader):\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    gen_data, mu, logvar = model(data)\n",
    "    gen_images = gen_data.view(-1, 1,28,28)\n",
    "    #print(data[0])\n",
    "    save_image(gen_images.data, 'images/gen{}.jpg'.format(index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CPU only training\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.8)\n",
    "def train(epoch):\n",
    "    classifier.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        gen_data, mu, logvar = model(data)\n",
    "        #Check reconstruction loss \n",
    "        recon_loss = reconstruction_function(gen_data, data)\n",
    "        #print(\"Reconstruction loss is\", recon_loss)\n",
    "        #print(\"Type of generated data\", type(gen_data))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = classifier(gen_data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test(epoch, valid_loader):\n",
    "    classifier.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        gen_data, mu, logvar = model(data)\n",
    "        output = classifier(gen_data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Classification phase\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sample Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testset = pickle.load(open(\"../data/kaggle/test.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(testset,batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test(1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_predict = np.array([])\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = model(data)\n",
    "    temp = output.data.max(1)[1].numpy().reshape(-1)\n",
    "    label_predict = np.concatenate((label_predict, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_label = pd.DataFrame(label_predict, columns=['label'], dtype=int)\n",
    "predict_label.reset_index(inplace=True)\n",
    "predict_label.rename(columns={'index': 'ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_label.to_csv('../data/kaggle/sample_submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
