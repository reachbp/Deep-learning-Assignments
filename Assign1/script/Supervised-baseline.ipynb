{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle \n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(trainset_new, batch_size=64, shuffle=True)\n",
    "# valid_loader = torch.utils.data.DataLoader(validset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing\n",
      "Done!\n",
      "3000\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "trainset_imoprt = pickle.load(open(\"../data/kaggle/train_labeled.p\", \"rb\"))\n",
    "validset_import = pickle.load(open(\"../data/kaggle/validation.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset_imoprt, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(validset_import, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d(0.3)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CPU only training\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test(epoch, valid_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3000 (0%)]\tLoss: 2.301911\n",
      "Train Epoch: 1 [640/3000 (21%)]\tLoss: 2.284214\n",
      "Train Epoch: 1 [1280/3000 (43%)]\tLoss: 2.280513\n",
      "Train Epoch: 1 [1920/3000 (64%)]\tLoss: 2.227513\n",
      "Train Epoch: 1 [2560/3000 (85%)]\tLoss: 2.171226\n",
      "\n",
      "Test set: Average loss: 2.0660, Accuracy: 4569/10000 (46%)\n",
      "\n",
      "Train Epoch: 2 [0/3000 (0%)]\tLoss: 2.135903\n",
      "Train Epoch: 2 [640/3000 (21%)]\tLoss: 1.993814\n",
      "Train Epoch: 2 [1280/3000 (43%)]\tLoss: 1.825414\n",
      "Train Epoch: 2 [1920/3000 (64%)]\tLoss: 1.794558\n",
      "Train Epoch: 2 [2560/3000 (85%)]\tLoss: 1.297441\n",
      "\n",
      "Test set: Average loss: 0.8214, Accuracy: 8037/10000 (80%)\n",
      "\n",
      "Train Epoch: 3 [0/3000 (0%)]\tLoss: 1.120000\n",
      "Train Epoch: 3 [640/3000 (21%)]\tLoss: 1.028890\n",
      "Train Epoch: 3 [1280/3000 (43%)]\tLoss: 1.012193\n",
      "Train Epoch: 3 [1920/3000 (64%)]\tLoss: 1.091102\n",
      "Train Epoch: 3 [2560/3000 (85%)]\tLoss: 0.842491\n",
      "\n",
      "Test set: Average loss: 0.4862, Accuracy: 8581/10000 (86%)\n",
      "\n",
      "Train Epoch: 4 [0/3000 (0%)]\tLoss: 0.867968\n",
      "Train Epoch: 4 [640/3000 (21%)]\tLoss: 0.617329\n",
      "Train Epoch: 4 [1280/3000 (43%)]\tLoss: 0.485976\n",
      "Train Epoch: 4 [1920/3000 (64%)]\tLoss: 0.704318\n",
      "Train Epoch: 4 [2560/3000 (85%)]\tLoss: 0.631890\n",
      "\n",
      "Test set: Average loss: 0.3564, Accuracy: 9026/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/3000 (0%)]\tLoss: 0.577270\n",
      "Train Epoch: 5 [640/3000 (21%)]\tLoss: 0.667569\n",
      "Train Epoch: 5 [1280/3000 (43%)]\tLoss: 0.454609\n",
      "Train Epoch: 5 [1920/3000 (64%)]\tLoss: 0.486148\n",
      "Train Epoch: 5 [2560/3000 (85%)]\tLoss: 0.443969\n",
      "\n",
      "Test set: Average loss: 0.2980, Accuracy: 9168/10000 (92%)\n",
      "\n",
      "Train Epoch: 6 [0/3000 (0%)]\tLoss: 0.496117\n",
      "Train Epoch: 6 [640/3000 (21%)]\tLoss: 0.654639\n",
      "Train Epoch: 6 [1280/3000 (43%)]\tLoss: 0.376835\n",
      "Train Epoch: 6 [1920/3000 (64%)]\tLoss: 0.506204\n",
      "Train Epoch: 6 [2560/3000 (85%)]\tLoss: 0.806875\n",
      "\n",
      "Test set: Average loss: 0.2686, Accuracy: 9256/10000 (93%)\n",
      "\n",
      "Train Epoch: 7 [0/3000 (0%)]\tLoss: 0.334643\n",
      "Train Epoch: 7 [640/3000 (21%)]\tLoss: 0.450423\n",
      "Train Epoch: 7 [1280/3000 (43%)]\tLoss: 0.416886\n",
      "Train Epoch: 7 [1920/3000 (64%)]\tLoss: 0.405198\n",
      "Train Epoch: 7 [2560/3000 (85%)]\tLoss: 0.716241\n",
      "\n",
      "Test set: Average loss: 0.2419, Accuracy: 9313/10000 (93%)\n",
      "\n",
      "Train Epoch: 8 [0/3000 (0%)]\tLoss: 0.532756\n",
      "Train Epoch: 8 [640/3000 (21%)]\tLoss: 0.430566\n",
      "Train Epoch: 8 [1280/3000 (43%)]\tLoss: 0.393121\n",
      "Train Epoch: 8 [1920/3000 (64%)]\tLoss: 0.277581\n",
      "Train Epoch: 8 [2560/3000 (85%)]\tLoss: 0.340598\n",
      "\n",
      "Test set: Average loss: 0.2261, Accuracy: 9295/10000 (93%)\n",
      "\n",
      "Train Epoch: 9 [0/3000 (0%)]\tLoss: 0.405708\n",
      "Train Epoch: 9 [640/3000 (21%)]\tLoss: 0.346699\n",
      "Train Epoch: 9 [1280/3000 (43%)]\tLoss: 0.426770\n",
      "Train Epoch: 9 [1920/3000 (64%)]\tLoss: 0.344479\n",
      "Train Epoch: 9 [2560/3000 (85%)]\tLoss: 0.418735\n",
      "\n",
      "Test set: Average loss: 0.2073, Accuracy: 9376/10000 (94%)\n",
      "\n",
      "Train Epoch: 10 [0/3000 (0%)]\tLoss: 0.173604\n",
      "Train Epoch: 10 [640/3000 (21%)]\tLoss: 0.336879\n",
      "Train Epoch: 10 [1280/3000 (43%)]\tLoss: 0.380114\n",
      "Train Epoch: 10 [1920/3000 (64%)]\tLoss: 0.506730\n",
      "Train Epoch: 10 [2560/3000 (85%)]\tLoss: 0.449345\n",
      "\n",
      "Test set: Average loss: 0.2101, Accuracy: 9384/10000 (94%)\n",
      "\n",
      "Train Epoch: 11 [0/3000 (0%)]\tLoss: 0.261699\n",
      "Train Epoch: 11 [640/3000 (21%)]\tLoss: 0.170412\n",
      "Train Epoch: 11 [1280/3000 (43%)]\tLoss: 0.335848\n",
      "Train Epoch: 11 [1920/3000 (64%)]\tLoss: 0.333631\n",
      "Train Epoch: 11 [2560/3000 (85%)]\tLoss: 0.467318\n",
      "\n",
      "Test set: Average loss: 0.1789, Accuracy: 9479/10000 (95%)\n",
      "\n",
      "Train Epoch: 12 [0/3000 (0%)]\tLoss: 0.420783\n",
      "Train Epoch: 12 [640/3000 (21%)]\tLoss: 0.233568\n",
      "Train Epoch: 12 [1280/3000 (43%)]\tLoss: 0.260410\n",
      "Train Epoch: 12 [1920/3000 (64%)]\tLoss: 0.406392\n",
      "Train Epoch: 12 [2560/3000 (85%)]\tLoss: 0.206185\n",
      "\n",
      "Test set: Average loss: 0.1849, Accuracy: 9429/10000 (94%)\n",
      "\n",
      "Train Epoch: 13 [0/3000 (0%)]\tLoss: 0.386086\n",
      "Train Epoch: 13 [640/3000 (21%)]\tLoss: 0.319711\n",
      "Train Epoch: 13 [1280/3000 (43%)]\tLoss: 0.202385\n",
      "Train Epoch: 13 [1920/3000 (64%)]\tLoss: 0.362045\n",
      "Train Epoch: 13 [2560/3000 (85%)]\tLoss: 0.169454\n",
      "\n",
      "Test set: Average loss: 0.1711, Accuracy: 9513/10000 (95%)\n",
      "\n",
      "Train Epoch: 14 [0/3000 (0%)]\tLoss: 0.206832\n",
      "Train Epoch: 14 [640/3000 (21%)]\tLoss: 0.383488\n",
      "Train Epoch: 14 [1280/3000 (43%)]\tLoss: 0.399637\n",
      "Train Epoch: 14 [1920/3000 (64%)]\tLoss: 0.280042\n",
      "Train Epoch: 14 [2560/3000 (85%)]\tLoss: 0.200646\n",
      "\n",
      "Test set: Average loss: 0.1658, Accuracy: 9531/10000 (95%)\n",
      "\n",
      "Train Epoch: 15 [0/3000 (0%)]\tLoss: 0.210380\n",
      "Train Epoch: 15 [640/3000 (21%)]\tLoss: 0.257025\n",
      "Train Epoch: 15 [1280/3000 (43%)]\tLoss: 0.167697\n",
      "Train Epoch: 15 [1920/3000 (64%)]\tLoss: 0.145165\n",
      "Train Epoch: 15 [2560/3000 (85%)]\tLoss: 0.307631\n",
      "\n",
      "Test set: Average loss: 0.1617, Accuracy: 9527/10000 (95%)\n",
      "\n",
      "Train Epoch: 16 [0/3000 (0%)]\tLoss: 0.435959\n",
      "Train Epoch: 16 [640/3000 (21%)]\tLoss: 0.412813\n",
      "Train Epoch: 16 [1280/3000 (43%)]\tLoss: 0.204243\n",
      "Train Epoch: 16 [1920/3000 (64%)]\tLoss: 0.371664\n",
      "Train Epoch: 16 [2560/3000 (85%)]\tLoss: 0.292460\n",
      "\n",
      "Test set: Average loss: 0.1603, Accuracy: 9533/10000 (95%)\n",
      "\n",
      "Train Epoch: 17 [0/3000 (0%)]\tLoss: 0.149643\n",
      "Train Epoch: 17 [640/3000 (21%)]\tLoss: 0.098717\n",
      "Train Epoch: 17 [1280/3000 (43%)]\tLoss: 0.331406\n",
      "Train Epoch: 17 [1920/3000 (64%)]\tLoss: 0.355223\n",
      "Train Epoch: 17 [2560/3000 (85%)]\tLoss: 0.312486\n",
      "\n",
      "Test set: Average loss: 0.1500, Accuracy: 9557/10000 (96%)\n",
      "\n",
      "Train Epoch: 18 [0/3000 (0%)]\tLoss: 0.350537\n",
      "Train Epoch: 18 [640/3000 (21%)]\tLoss: 0.269542\n",
      "Train Epoch: 18 [1280/3000 (43%)]\tLoss: 0.177841\n",
      "Train Epoch: 18 [1920/3000 (64%)]\tLoss: 0.375048\n",
      "Train Epoch: 18 [2560/3000 (85%)]\tLoss: 0.147311\n",
      "\n",
      "Test set: Average loss: 0.1518, Accuracy: 9551/10000 (96%)\n",
      "\n",
      "Train Epoch: 19 [0/3000 (0%)]\tLoss: 0.171173\n",
      "Train Epoch: 19 [640/3000 (21%)]\tLoss: 0.155640\n",
      "Train Epoch: 19 [1280/3000 (43%)]\tLoss: 0.358680\n",
      "Train Epoch: 19 [1920/3000 (64%)]\tLoss: 0.227234\n",
      "Train Epoch: 19 [2560/3000 (85%)]\tLoss: 0.324744\n",
      "\n",
      "Test set: Average loss: 0.1414, Accuracy: 9582/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 20):\n",
    "    train(epoch)\n",
    "    test(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sample Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testset = pickle.load(open(\"../data/kaggle/test.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(testset,batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1179, Accuracy: 9635/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_predict = np.array([])\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = model(data)\n",
    "    temp = output.data.max(1)[1].numpy().reshape(-1)\n",
    "    label_predict = np.concatenate((label_predict, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.,  2.,  1., ...,  4.,  5.,  6.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "predict_label = pd.DataFrame(label_predict, columns=['label'], dtype=int)\n",
    "predict_label.reset_index(inplace=True)\n",
    "predict_label.rename(columns={'index': 'ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  label\n",
       "0   0      7\n",
       "1   1      2\n",
       "2   2      1\n",
       "3   3      0\n",
       "4   4      4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_label.to_csv('../data/kaggle/sample_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
