{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                ])\n",
    "trainset_labeled_import = pickle.load(open(\"../data/kaggle/train_labeled.p\", \"rb\"))\n",
    "trainset_unlabeled_import = pickle.load(open(\"../data/kaggle/train_unlabeled.p\", \"rb\"))\n",
    "validset_import = pickle.load(open(\"../data/kaggle/validation.p\", \"rb\"))\n",
    "train_labeled_loader = torch.utils.data.DataLoader(trainset_labeled_import, batch_size=32, shuffle=True)\n",
    "train_unlabeled_loader = torch.utils.data.DataLoader(trainset_unlabeled_import, batch_size=256,\n",
    "                                                     shuffle=True)\n",
    "\n",
    "train_unlabeled_loader.dataset.train_labels = [-1 for i in range(len(train_unlabeled_loader.dataset.train_data))]\n",
    "valid_loader = torch.utils.data.DataLoader(validset_import, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.4230 -2.0857 -2.4230 -2.4230 -2.0699 -2.4230 -2.3342 -2.2204 -2.4225 -2.2946\n",
       "-2.3742 -2.1285 -2.3742 -2.3742 -2.1718 -2.2952 -2.3677 -2.3742 -2.3742 -2.2335\n",
       "-2.3802 -2.0872 -2.3802 -2.3778 -2.2749 -2.1184 -2.3277 -2.3802 -2.3802 -2.3802\n",
       "-2.3835 -2.0849 -2.3227 -2.3835 -2.2288 -2.2904 -2.3030 -2.3835 -2.3835 -2.3031\n",
       "-2.3603 -2.1602 -2.3546 -2.3603 -2.1501 -2.3465 -2.3603 -2.3174 -2.3603 -2.2879\n",
       "-2.4241 -1.9623 -2.3352 -2.4241 -2.3873 -2.2147 -2.2514 -2.2815 -2.4241 -2.4241\n",
       "-2.4228 -2.1905 -2.1599 -2.4228 -1.9354 -2.4228 -2.3395 -2.4228 -2.4228 -2.4228\n",
       "-2.3715 -2.3715 -2.3715 -2.1984 -1.9549 -2.3715 -2.3637 -2.3715 -2.3715 -2.3715\n",
       "-2.3742 -2.2783 -2.3742 -2.3090 -1.9508 -2.3742 -2.3282 -2.3742 -2.3742 -2.3742\n",
       "-2.4529 -2.0989 -2.3338 -2.4529 -2.2214 -2.1298 -2.3025 -2.4529 -2.4529 -2.2167\n",
       "-2.3855 -2.0893 -2.3855 -2.3855 -2.2527 -2.3422 -2.3047 -2.2136 -2.3855 -2.3255\n",
       "-2.4127 -2.3684 -2.4127 -2.4096 -2.0278 -2.4127 -1.9314 -2.4127 -2.4127 -2.3853\n",
       "-2.3540 -2.1958 -2.4034 -2.4034 -1.9802 -2.1783 -2.4034 -2.4034 -2.4034 -2.4034\n",
       "-2.3301 -2.2255 -2.3104 -2.3301 -2.1982 -2.3272 -2.3301 -2.3301 -2.3301 -2.3252\n",
       "-2.3914 -2.2005 -2.3914 -2.3327 -2.0005 -2.3914 -2.2530 -2.3914 -2.3608 -2.3914\n",
       "-2.3990 -2.3766 -2.3990 -2.3990 -2.0366 -2.3990 -2.0230 -2.3990 -2.3990 -2.3070\n",
       "-2.4359 -2.2112 -2.4359 -2.4359 -1.8450 -2.4359 -2.2284 -2.4359 -2.4359 -2.3069\n",
       "-2.3629 -2.0412 -2.3629 -2.3629 -2.3629 -2.3388 -2.2788 -2.2934 -2.3064 -2.3629\n",
       "-2.4091 -2.0666 -2.2852 -2.4091 -1.9494 -2.4091 -2.4091 -2.4091 -2.4091 -2.4091\n",
       "-2.3599 -2.0897 -2.3421 -2.3599 -2.1931 -2.3207 -2.3206 -2.3599 -2.3599 -2.3599\n",
       "-2.4295 -1.9548 -2.4295 -2.4295 -2.3116 -2.1571 -2.3576 -2.2728 -2.4295 -2.3685\n",
       "-2.3889 -2.1526 -2.3889 -2.3889 -2.0122 -2.3889 -2.3889 -2.3399 -2.2691 -2.3889\n",
       "-2.4485 -1.9393 -2.4485 -2.4485 -2.4057 -2.1071 -2.1430 -2.4485 -2.3542 -2.4485\n",
       "-2.4483 -2.2459 -2.3486 -2.4483 -1.8791 -2.3733 -2.3335 -2.2227 -2.4483 -2.4285\n",
       "-2.3647 -2.0698 -2.3647 -2.3647 -2.2937 -2.1620 -2.3647 -2.3647 -2.3647 -2.3647\n",
       "-2.2909 -2.2694 -2.1284 -2.3871 -2.1055 -2.3871 -2.3521 -2.3871 -2.3871 -2.3871\n",
       "-2.4671 -1.8734 -2.3451 -2.4671 -2.2038 -2.3729 -2.2874 -2.4671 -2.3633 -2.3345\n",
       "-2.4060 -2.0427 -2.4060 -2.4060 -2.4060 -2.2124 -2.2266 -2.1899 -2.4060 -2.4060\n",
       "-2.4168 -2.0531 -2.3449 -2.4168 -2.2925 -2.1215 -2.2540 -2.4168 -2.4168 -2.3738\n",
       "-2.4665 -1.8498 -2.4840 -2.4840 -2.2633 -2.1762 -2.4840 -2.3559 -2.3923 -2.2640\n",
       "-2.4323 -2.4323 -2.4323 -2.4323 -1.8436 -2.4323 -2.2551 -2.2725 -2.4323 -2.2367\n",
       "-2.3765 -2.2604 -2.3765 -2.3765 -2.2064 -2.2133 -2.1286 -2.3765 -2.3765 -2.3765\n",
       "[torch.FloatTensor of size 32x10]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d(0.3)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        #print(x.size())\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x)\n",
    "model = Net()\n",
    "a = torch.randn(32, 1, 28, 28)\n",
    "model.forward(Variable(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.3885 -2.3885 -2.3885 -2.3885 -2.2074 -2.1908 -2.3885 -2.3885 -2.3885 -1.9966\n",
       "-2.3157 -2.3157 -2.3013 -2.3157 -2.3157 -2.3157 -2.3157 -2.3157 -2.3157 -2.2044\n",
       "-2.3847 -2.3847 -2.3847 -2.3847 -2.3421 -2.3847 -2.3847 -2.0694 -2.2659 -2.1107\n",
       "-2.4014 -2.3823 -2.4014 -2.4014 -2.3507 -2.1476 -2.0121 -2.4014 -2.3287 -2.2820\n",
       "-2.3900 -2.3900 -2.3900 -2.2110 -2.3900 -2.2174 -2.3900 -2.3900 -1.9653 -2.3900\n",
       "-2.4278 -2.4278 -2.3567 -2.4278 -2.4278 -2.0843 -2.3407 -2.4278 -2.2954 -1.9469\n",
       "-2.3720 -2.3720 -2.3720 -2.3720 -2.3720 -2.2710 -2.2748 -2.0849 -2.3720 -2.2082\n",
       "-2.1829 -2.3394 -2.3394 -2.3394 -2.3076 -2.3394 -2.2784 -2.3394 -2.2605 -2.3115\n",
       "-2.3772 -2.3772 -2.3772 -2.2878 -2.3772 -2.1622 -2.3428 -2.2212 -2.3772 -2.1643\n",
       "-2.1357 -2.4849 -2.4849 -2.0544 -2.4335 -2.4849 -2.2952 -2.2487 -2.1664 -2.3530\n",
       "-2.4213 -2.4213 -2.4213 -2.4213 -2.0300 -2.4213 -2.4165 -2.4213 -1.9950 -2.2021\n",
       "-2.3539 -2.3599 -2.3599 -2.3599 -2.0902 -2.1211 -2.3599 -2.3599 -2.3556 -2.3599\n",
       "-2.4513 -2.4910 -2.1423 -2.0874 -2.2723 -2.3258 -2.4910 -2.4910 -2.4910 -1.9638\n",
       "-2.5551 -2.3564 -2.5551 -2.5551 -2.5551 -1.6685 -2.0153 -2.5551 -2.1427 -2.5551\n",
       "-2.2927 -2.3906 -2.3648 -2.3906 -2.0864 -2.3906 -2.3906 -2.3906 -2.0292 -2.3906\n",
       "-2.3179 -2.3179 -2.3179 -2.3179 -2.3179 -2.2613 -2.3179 -2.3179 -2.3179 -2.2262\n",
       "-2.3827 -2.3827 -2.3311 -2.2691 -2.1885 -2.3353 -2.3827 -2.3827 -2.0777 -2.3421\n",
       "-2.4495 -2.4495 -2.3570 -2.4495 -2.4495 -1.9995 -2.4062 -2.4495 -2.1200 -2.0557\n",
       "-2.3794 -2.3794 -2.3794 -2.3475 -2.3794 -2.2711 -2.3794 -2.1474 -2.3794 -2.0496\n",
       "-2.1975 -2.2616 -2.3044 -2.3586 -2.3593 -2.3817 -2.3817 -2.3817 -2.0691 -2.3817\n",
       "-2.3418 -2.3418 -2.3418 -2.3418 -2.3418 -2.3418 -2.3418 -2.3418 -2.3403 -2.0066\n",
       "-2.4289 -2.4289 -2.3316 -2.3377 -2.1046 -2.4063 -2.4289 -2.4289 -2.1582 -2.0698\n",
       "-2.3309 -2.3309 -2.3309 -2.3309 -2.2557 -2.2584 -2.3309 -2.3309 -2.3309 -2.2051\n",
       "-2.3746 -2.3746 -2.3746 -2.3746 -2.0428 -2.1736 -2.3546 -2.3746 -2.2697 -2.3746\n",
       "-2.4709 -2.4709 -2.4709 -2.1513 -2.3478 -2.4709 -2.3529 -2.3394 -2.1617 -1.9420\n",
       "-2.3561 -2.3561 -2.3561 -2.3561 -2.0001 -2.3561 -2.3561 -2.3561 -2.2407 -2.3561\n",
       "-2.4410 -2.4410 -2.4410 -2.4410 -1.8328 -2.2095 -2.2034 -2.3405 -2.4410 -2.4282\n",
       "-2.3158 -2.3730 -2.3730 -2.3730 -2.0007 -2.3730 -2.3730 -2.3730 -2.3703 -2.1768\n",
       "-2.4488 -2.4488 -2.4488 -2.2656 -2.1965 -2.3930 -2.0363 -2.4488 -2.0312 -2.4488\n",
       "-2.3595 -2.3595 -2.3595 -2.3595 -2.3117 -2.1590 -2.3595 -2.3595 -2.3595 -2.0859\n",
       "-2.4545 -2.4545 -2.4545 -2.0278 -2.4545 -2.0656 -2.4545 -2.4545 -2.4545 -1.9641\n",
       "-2.3979 -2.2771 -2.3979 -2.3979 -2.3979 -2.2969 -2.0704 -2.3737 -2.1910 -2.2815\n",
       "[torch.FloatTensor of size 32x10]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NetBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetBN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=0, padding=0, ceil_mode=True) # change\n",
    "\n",
    "        self.conv1_bn = nn.BatchNorm2d(64)\n",
    "        self.conv2_bn = nn.BatchNorm2d(128)\n",
    "        self.conv3_bn = nn.BatchNorm2d(256)\n",
    "        self.conv1_drop = nn.Dropout2d(0.2)\n",
    "        self.conv2_drop = nn.Dropout2d(0.3)\n",
    "        self.conv3_drop = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(32 * 8, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1_drop(F.relu(F.max_pool2d(self.conv1_bn(self.conv1(x)), (2,2))))\n",
    "\n",
    "        x = self.conv2_drop(F.relu(F.max_pool2d(self.conv2_bn(self.conv2(x)), (2,2))))\n",
    "\n",
    "        x = self.conv3_drop(F.relu(F.max_pool2d(self.conv3_bn(self.conv3(x)), (2,2))))\n",
    "        x = x.view(-1, 32 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = NetBN()\n",
    "a = torch.randn(32, 1, 28, 28)\n",
    "model.forward(Variable(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = pickle.load(open(\"model.p\", \"rb\"))\n",
    "optimizer = optim.SGD(model.parameters(), lr=1.5, momentum=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "my_criterion = MyCriterion()\n",
    "x = Variable(torch.randn(11, 10).type(torch.FloatTensor))\n",
    "y = Variable(torch.range(1,6, .5).type(torch.LongTensor))\n",
    "\n",
    "a = torch.from_numpy(np.array([0]))\n",
    "b = torch.from_numpy(np.array([1]))\n",
    "c = torch.from_numpy(np.array([10.0]))\n",
    "\n",
    "print(x)\n",
    "# print(torch.from_numpy(np.array([10])))\n",
    "my_criterion.train()\n",
    "\n",
    "first_loss  = my_criterion(x, y, Variable(c.float()),  Variable(a))\n",
    "\n",
    "second_loss = my_criterion(x, y, Variable(c.float()),  Variable(b))\n",
    "print(first_loss, second_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_labeled(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_labeled_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Labeled Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_labeled_loader.dataset),\n",
    "                       100. * batch_idx / len(train_labeled_loader), loss.data[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_unlabeled(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,_) in enumerate(train_unlabeled_loader):\n",
    "        data = Variable(data)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        target = Variable(torch.LongTensor(output.data.max(1)[1].numpy().reshape(-1)))\n",
    "#         loss = my_criterion.forward(output, target, Variable(torch.LongTensor(epoch)), Variable(torch.LongTensor(2)))\n",
    "#         my_criterion.backward(loss)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Unlabeled Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_unlabeled_loader.dataset),\n",
    "                       100. * batch_idx / len(train_unlabeled_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(epoch, valid_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target)\n",
    "        \n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss.data.numpy()[0], correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labeled Epoch: 1 [0/3000 (0%)]\tLoss: 2.250507\n",
      "Train Labeled Epoch: 1 [320/3000 (11%)]\tLoss: 2.306233\n",
      "Train Labeled Epoch: 1 [640/3000 (21%)]\tLoss: 2.251914\n",
      "Train Labeled Epoch: 1 [960/3000 (32%)]\tLoss: 2.073837\n",
      "Train Labeled Epoch: 1 [1280/3000 (43%)]\tLoss: 1.868223\n",
      "Train Labeled Epoch: 1 [1600/3000 (53%)]\tLoss: 1.891294\n",
      "Train Labeled Epoch: 1 [1920/3000 (64%)]\tLoss: 1.425283\n",
      "Train Labeled Epoch: 1 [2240/3000 (74%)]\tLoss: 1.460097\n",
      "Train Labeled Epoch: 1 [2560/3000 (85%)]\tLoss: 1.130306\n",
      "Train Labeled Epoch: 1 [2880/3000 (96%)]\tLoss: 0.732923\n",
      "\n",
      "Test set: Average loss: 0.5256, Accuracy: 8822/10000 (88%)\n",
      "\n",
      "Train Labeled Epoch: 2 [0/3000 (0%)]\tLoss: 0.757906\n",
      "Train Labeled Epoch: 2 [320/3000 (11%)]\tLoss: 0.556363\n",
      "Train Labeled Epoch: 2 [640/3000 (21%)]\tLoss: 0.704225\n",
      "Train Labeled Epoch: 2 [960/3000 (32%)]\tLoss: 0.554556\n",
      "Train Labeled Epoch: 2 [1280/3000 (43%)]\tLoss: 0.683406\n",
      "Train Labeled Epoch: 2 [1600/3000 (53%)]\tLoss: 0.478517\n",
      "Train Labeled Epoch: 2 [1920/3000 (64%)]\tLoss: 0.380870\n",
      "Train Labeled Epoch: 2 [2240/3000 (74%)]\tLoss: 0.387358\n",
      "Train Labeled Epoch: 2 [2560/3000 (85%)]\tLoss: 0.441075\n",
      "Train Labeled Epoch: 2 [2880/3000 (96%)]\tLoss: 0.481093\n",
      "\n",
      "Test set: Average loss: 0.2239, Accuracy: 9336/10000 (93%)\n",
      "\n",
      "Train Labeled Epoch: 3 [0/3000 (0%)]\tLoss: 0.351725\n",
      "Train Labeled Epoch: 3 [320/3000 (11%)]\tLoss: 0.345120\n",
      "Train Labeled Epoch: 3 [640/3000 (21%)]\tLoss: 0.473220\n",
      "Train Labeled Epoch: 3 [960/3000 (32%)]\tLoss: 0.245059\n",
      "Train Labeled Epoch: 3 [1280/3000 (43%)]\tLoss: 0.248515\n",
      "Train Labeled Epoch: 3 [1600/3000 (53%)]\tLoss: 0.561528\n",
      "Train Labeled Epoch: 3 [1920/3000 (64%)]\tLoss: 0.288475\n",
      "Train Labeled Epoch: 3 [2240/3000 (74%)]\tLoss: 0.599066\n",
      "Train Labeled Epoch: 3 [2560/3000 (85%)]\tLoss: 0.261227\n",
      "Train Labeled Epoch: 3 [2880/3000 (96%)]\tLoss: 0.139056\n",
      "\n",
      "Test set: Average loss: 0.1781, Accuracy: 9440/10000 (94%)\n",
      "\n",
      "Train Labeled Epoch: 4 [0/3000 (0%)]\tLoss: 0.178201\n",
      "Train Labeled Epoch: 4 [320/3000 (11%)]\tLoss: 0.157605\n",
      "Train Labeled Epoch: 4 [640/3000 (21%)]\tLoss: 0.267393\n",
      "Train Labeled Epoch: 4 [960/3000 (32%)]\tLoss: 0.861508\n",
      "Train Labeled Epoch: 4 [1280/3000 (43%)]\tLoss: 0.143318\n",
      "Train Labeled Epoch: 4 [1600/3000 (53%)]\tLoss: 0.099504\n",
      "Train Labeled Epoch: 4 [1920/3000 (64%)]\tLoss: 0.347246\n",
      "Train Labeled Epoch: 4 [2240/3000 (74%)]\tLoss: 0.169402\n",
      "Train Labeled Epoch: 4 [2560/3000 (85%)]\tLoss: 0.189402\n",
      "Train Labeled Epoch: 4 [2880/3000 (96%)]\tLoss: 0.296957\n",
      "\n",
      "Test set: Average loss: 0.1335, Accuracy: 9613/10000 (96%)\n",
      "\n",
      "Train Labeled Epoch: 5 [0/3000 (0%)]\tLoss: 0.302014\n",
      "Train Labeled Epoch: 5 [320/3000 (11%)]\tLoss: 0.306010\n",
      "Train Labeled Epoch: 5 [640/3000 (21%)]\tLoss: 0.349746\n",
      "Train Labeled Epoch: 5 [960/3000 (32%)]\tLoss: 0.103076\n",
      "Train Labeled Epoch: 5 [1280/3000 (43%)]\tLoss: 0.093528\n",
      "Train Labeled Epoch: 5 [1600/3000 (53%)]\tLoss: 0.175743\n",
      "Train Labeled Epoch: 5 [1920/3000 (64%)]\tLoss: 0.107557\n",
      "Train Labeled Epoch: 5 [2240/3000 (74%)]\tLoss: 0.128461\n",
      "Train Labeled Epoch: 5 [2560/3000 (85%)]\tLoss: 0.213295\n",
      "Train Labeled Epoch: 5 [2880/3000 (96%)]\tLoss: 0.147831\n",
      "\n",
      "Test set: Average loss: 0.1147, Accuracy: 9660/10000 (97%)\n",
      "\n",
      "Train Labeled Epoch: 6 [0/3000 (0%)]\tLoss: 0.078439\n",
      "Train Labeled Epoch: 6 [320/3000 (11%)]\tLoss: 0.314156\n",
      "Train Labeled Epoch: 6 [640/3000 (21%)]\tLoss: 0.050552\n",
      "Train Labeled Epoch: 6 [960/3000 (32%)]\tLoss: 0.066238\n",
      "Train Labeled Epoch: 6 [1280/3000 (43%)]\tLoss: 0.099046\n",
      "Train Labeled Epoch: 6 [1600/3000 (53%)]\tLoss: 0.187124\n",
      "Train Labeled Epoch: 6 [1920/3000 (64%)]\tLoss: 0.197544\n",
      "Train Labeled Epoch: 6 [2240/3000 (74%)]\tLoss: 0.106355\n",
      "Train Labeled Epoch: 6 [2560/3000 (85%)]\tLoss: 0.224488\n",
      "Train Labeled Epoch: 6 [2880/3000 (96%)]\tLoss: 0.388638\n",
      "\n",
      "Test set: Average loss: 0.1075, Accuracy: 9671/10000 (97%)\n",
      "\n",
      "Train Labeled Epoch: 7 [0/3000 (0%)]\tLoss: 0.256280\n",
      "Train Labeled Epoch: 7 [320/3000 (11%)]\tLoss: 0.131972\n",
      "Train Labeled Epoch: 7 [640/3000 (21%)]\tLoss: 0.462242\n",
      "Train Labeled Epoch: 7 [960/3000 (32%)]\tLoss: 0.097450\n",
      "Train Labeled Epoch: 7 [1280/3000 (43%)]\tLoss: 0.294778\n",
      "Train Labeled Epoch: 7 [1600/3000 (53%)]\tLoss: 0.075171\n",
      "Train Labeled Epoch: 7 [1920/3000 (64%)]\tLoss: 0.191409\n",
      "Train Labeled Epoch: 7 [2240/3000 (74%)]\tLoss: 0.054554\n",
      "Train Labeled Epoch: 7 [2560/3000 (85%)]\tLoss: 0.101596\n",
      "Train Labeled Epoch: 7 [2880/3000 (96%)]\tLoss: 0.093372\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9697/10000 (97%)\n",
      "\n",
      "Train Labeled Epoch: 8 [0/3000 (0%)]\tLoss: 0.224119\n",
      "Train Labeled Epoch: 8 [320/3000 (11%)]\tLoss: 0.252596\n",
      "Train Labeled Epoch: 8 [640/3000 (21%)]\tLoss: 0.062970\n",
      "Train Labeled Epoch: 8 [960/3000 (32%)]\tLoss: 0.188055\n",
      "Train Labeled Epoch: 8 [1280/3000 (43%)]\tLoss: 0.078365\n",
      "Train Labeled Epoch: 8 [1600/3000 (53%)]\tLoss: 0.085786\n",
      "Train Labeled Epoch: 8 [1920/3000 (64%)]\tLoss: 0.143511\n",
      "Train Labeled Epoch: 8 [2240/3000 (74%)]\tLoss: 0.119512\n",
      "Train Labeled Epoch: 8 [2560/3000 (85%)]\tLoss: 0.031131\n",
      "Train Labeled Epoch: 8 [2880/3000 (96%)]\tLoss: 0.110270\n",
      "\n",
      "Test set: Average loss: 0.0978, Accuracy: 9701/10000 (97%)\n",
      "\n",
      "Train Labeled Epoch: 9 [0/3000 (0%)]\tLoss: 0.134284\n",
      "Train Labeled Epoch: 9 [320/3000 (11%)]\tLoss: 0.051754\n",
      "Train Labeled Epoch: 9 [640/3000 (21%)]\tLoss: 0.164802\n",
      "Train Labeled Epoch: 9 [960/3000 (32%)]\tLoss: 0.046943\n",
      "Train Labeled Epoch: 9 [1280/3000 (43%)]\tLoss: 0.110825\n",
      "Train Labeled Epoch: 9 [1600/3000 (53%)]\tLoss: 0.042434\n",
      "Train Labeled Epoch: 9 [1920/3000 (64%)]\tLoss: 0.015204\n",
      "Train Labeled Epoch: 9 [2240/3000 (74%)]\tLoss: 0.061129\n",
      "Train Labeled Epoch: 9 [2560/3000 (85%)]\tLoss: 0.070148\n",
      "Train Labeled Epoch: 9 [2880/3000 (96%)]\tLoss: 0.180755\n",
      "\n",
      "Test set: Average loss: 0.0874, Accuracy: 9728/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    validate(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump(model, open(\"model.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = pickle.load(open(\"model.p\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labeled Epoch: 1 [0/3000 (0%)]\tLoss: 0.304256\n",
      "Train Labeled Epoch: 1 [320/3000 (11%)]\tLoss: 0.153869\n",
      "Train Labeled Epoch: 1 [640/3000 (21%)]\tLoss: 0.072204\n",
      "Train Labeled Epoch: 1 [960/3000 (32%)]\tLoss: 0.153059\n",
      "Train Labeled Epoch: 1 [1280/3000 (43%)]\tLoss: 0.019983\n",
      "Train Labeled Epoch: 1 [1600/3000 (53%)]\tLoss: 0.409640\n",
      "Train Labeled Epoch: 1 [1920/3000 (64%)]\tLoss: 0.058668\n",
      "Train Labeled Epoch: 1 [2240/3000 (74%)]\tLoss: 0.048152\n",
      "Train Labeled Epoch: 1 [2560/3000 (85%)]\tLoss: 0.191246\n",
      "Train Labeled Epoch: 1 [2880/3000 (96%)]\tLoss: 0.031356\n",
      "Train Unlabeled Epoch: 1 [0/47000 (0%)]\tLoss: 0.081837\n",
      "Train Unlabeled Epoch: 1 [2560/47000 (5%)]\tLoss: 0.078756\n",
      "Train Unlabeled Epoch: 1 [5120/47000 (11%)]\tLoss: 0.079416\n",
      "Train Unlabeled Epoch: 1 [7680/47000 (16%)]\tLoss: 0.065487\n",
      "Train Unlabeled Epoch: 1 [10240/47000 (22%)]\tLoss: 0.082107\n",
      "Train Unlabeled Epoch: 1 [12800/47000 (27%)]\tLoss: 0.062613\n",
      "Train Unlabeled Epoch: 1 [15360/47000 (33%)]\tLoss: 0.069898\n",
      "Train Unlabeled Epoch: 1 [17920/47000 (38%)]\tLoss: 0.079728\n",
      "Train Unlabeled Epoch: 1 [20480/47000 (43%)]\tLoss: 0.068075\n",
      "Train Unlabeled Epoch: 1 [23040/47000 (49%)]\tLoss: 0.062845\n",
      "Train Unlabeled Epoch: 1 [25600/47000 (54%)]\tLoss: 0.066269\n",
      "Train Unlabeled Epoch: 1 [28160/47000 (60%)]\tLoss: 0.072295\n",
      "Train Unlabeled Epoch: 1 [30720/47000 (65%)]\tLoss: 0.059495\n",
      "Train Unlabeled Epoch: 1 [33280/47000 (71%)]\tLoss: 0.059010\n",
      "Train Unlabeled Epoch: 1 [35840/47000 (76%)]\tLoss: 0.071763\n",
      "Train Unlabeled Epoch: 1 [38400/47000 (82%)]\tLoss: 0.058722\n",
      "Train Unlabeled Epoch: 1 [40960/47000 (87%)]\tLoss: 0.071906\n",
      "Train Unlabeled Epoch: 1 [43520/47000 (92%)]\tLoss: 0.062834\n",
      "Train Unlabeled Epoch: 1 [46080/47000 (98%)]\tLoss: 0.050534\n",
      "\n",
      "Test set: Average loss: 0.0874, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Labeled Epoch: 2 [0/3000 (0%)]\tLoss: 0.017962\n",
      "Train Labeled Epoch: 2 [320/3000 (11%)]\tLoss: 0.067169\n",
      "Train Labeled Epoch: 2 [640/3000 (21%)]\tLoss: 0.016670\n",
      "Train Labeled Epoch: 2 [960/3000 (32%)]\tLoss: 0.085477\n",
      "Train Labeled Epoch: 2 [1280/3000 (43%)]\tLoss: 0.137040\n",
      "Train Labeled Epoch: 2 [1600/3000 (53%)]\tLoss: 0.017759\n",
      "Train Labeled Epoch: 2 [1920/3000 (64%)]\tLoss: 0.071220\n",
      "Train Labeled Epoch: 2 [2240/3000 (74%)]\tLoss: 0.096499\n",
      "Train Labeled Epoch: 2 [2560/3000 (85%)]\tLoss: 0.201982\n",
      "Train Labeled Epoch: 2 [2880/3000 (96%)]\tLoss: 0.071947\n",
      "Train Unlabeled Epoch: 2 [0/47000 (0%)]\tLoss: 0.053120\n",
      "Train Unlabeled Epoch: 2 [2560/47000 (5%)]\tLoss: 0.067501\n",
      "Train Unlabeled Epoch: 2 [5120/47000 (11%)]\tLoss: 0.070619\n",
      "Train Unlabeled Epoch: 2 [7680/47000 (16%)]\tLoss: 0.076783\n",
      "Train Unlabeled Epoch: 2 [10240/47000 (22%)]\tLoss: 0.059333\n",
      "Train Unlabeled Epoch: 2 [12800/47000 (27%)]\tLoss: 0.061799\n",
      "Train Unlabeled Epoch: 2 [15360/47000 (33%)]\tLoss: 0.069040\n",
      "Train Unlabeled Epoch: 2 [17920/47000 (38%)]\tLoss: 0.062862\n",
      "Train Unlabeled Epoch: 2 [20480/47000 (43%)]\tLoss: 0.073394\n",
      "Train Unlabeled Epoch: 2 [23040/47000 (49%)]\tLoss: 0.049947\n",
      "Train Unlabeled Epoch: 2 [25600/47000 (54%)]\tLoss: 0.054933\n",
      "Train Unlabeled Epoch: 2 [28160/47000 (60%)]\tLoss: 0.055902\n",
      "Train Unlabeled Epoch: 2 [30720/47000 (65%)]\tLoss: 0.073807\n",
      "Train Unlabeled Epoch: 2 [33280/47000 (71%)]\tLoss: 0.063722\n",
      "Train Unlabeled Epoch: 2 [35840/47000 (76%)]\tLoss: 0.066792\n",
      "Train Unlabeled Epoch: 2 [38400/47000 (82%)]\tLoss: 0.065585\n",
      "Train Unlabeled Epoch: 2 [40960/47000 (87%)]\tLoss: 0.060763\n",
      "Train Unlabeled Epoch: 2 [43520/47000 (92%)]\tLoss: 0.090913\n",
      "Train Unlabeled Epoch: 2 [46080/47000 (98%)]\tLoss: 0.084044\n",
      "\n",
      "Test set: Average loss: 0.0877, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Labeled Epoch: 3 [0/3000 (0%)]\tLoss: 0.073023\n",
      "Train Labeled Epoch: 3 [320/3000 (11%)]\tLoss: 0.061372\n",
      "Train Labeled Epoch: 3 [640/3000 (21%)]\tLoss: 0.193909\n",
      "Train Labeled Epoch: 3 [960/3000 (32%)]\tLoss: 0.237445\n",
      "Train Labeled Epoch: 3 [1280/3000 (43%)]\tLoss: 0.084336\n",
      "Train Labeled Epoch: 3 [1600/3000 (53%)]\tLoss: 0.059578\n",
      "Train Labeled Epoch: 3 [1920/3000 (64%)]\tLoss: 0.241954\n",
      "Train Labeled Epoch: 3 [2240/3000 (74%)]\tLoss: 0.068574\n",
      "Train Labeled Epoch: 3 [2560/3000 (85%)]\tLoss: 0.247356\n",
      "Train Labeled Epoch: 3 [2880/3000 (96%)]\tLoss: 0.110948\n",
      "Train Unlabeled Epoch: 3 [0/47000 (0%)]\tLoss: 0.065313\n",
      "Train Unlabeled Epoch: 3 [2560/47000 (5%)]\tLoss: 0.079224\n",
      "Train Unlabeled Epoch: 3 [5120/47000 (11%)]\tLoss: 0.079514\n",
      "Train Unlabeled Epoch: 3 [7680/47000 (16%)]\tLoss: 0.083274\n",
      "Train Unlabeled Epoch: 3 [10240/47000 (22%)]\tLoss: 0.059198\n",
      "Train Unlabeled Epoch: 3 [12800/47000 (27%)]\tLoss: 0.061253\n",
      "Train Unlabeled Epoch: 3 [15360/47000 (33%)]\tLoss: 0.067927\n",
      "Train Unlabeled Epoch: 3 [17920/47000 (38%)]\tLoss: 0.083332\n",
      "Train Unlabeled Epoch: 3 [20480/47000 (43%)]\tLoss: 0.080120\n",
      "Train Unlabeled Epoch: 3 [23040/47000 (49%)]\tLoss: 0.080099\n",
      "Train Unlabeled Epoch: 3 [25600/47000 (54%)]\tLoss: 0.069484\n",
      "Train Unlabeled Epoch: 3 [28160/47000 (60%)]\tLoss: 0.079410\n",
      "Train Unlabeled Epoch: 3 [30720/47000 (65%)]\tLoss: 0.076176\n",
      "Train Unlabeled Epoch: 3 [33280/47000 (71%)]\tLoss: 0.074097\n",
      "Train Unlabeled Epoch: 3 [35840/47000 (76%)]\tLoss: 0.051405\n",
      "Train Unlabeled Epoch: 3 [38400/47000 (82%)]\tLoss: 0.059082\n",
      "Train Unlabeled Epoch: 3 [40960/47000 (87%)]\tLoss: 0.093661\n",
      "Train Unlabeled Epoch: 3 [43520/47000 (92%)]\tLoss: 0.060367\n",
      "Train Unlabeled Epoch: 3 [46080/47000 (98%)]\tLoss: 0.075283\n",
      "\n",
      "Test set: Average loss: 0.0882, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Labeled Epoch: 4 [0/3000 (0%)]\tLoss: 0.165005\n",
      "Train Labeled Epoch: 4 [320/3000 (11%)]\tLoss: 0.111354\n",
      "Train Labeled Epoch: 4 [640/3000 (21%)]\tLoss: 0.471173\n",
      "Train Labeled Epoch: 4 [960/3000 (32%)]\tLoss: 0.048829\n",
      "Train Labeled Epoch: 4 [1280/3000 (43%)]\tLoss: 0.026093\n",
      "Train Labeled Epoch: 4 [1600/3000 (53%)]\tLoss: 0.067323\n",
      "Train Labeled Epoch: 4 [1920/3000 (64%)]\tLoss: 0.186696\n",
      "Train Labeled Epoch: 4 [2240/3000 (74%)]\tLoss: 0.042407\n",
      "Train Labeled Epoch: 4 [2560/3000 (85%)]\tLoss: 0.025694\n",
      "Train Labeled Epoch: 4 [2880/3000 (96%)]\tLoss: 0.029770\n",
      "Train Unlabeled Epoch: 4 [0/47000 (0%)]\tLoss: 0.083872\n",
      "Train Unlabeled Epoch: 4 [2560/47000 (5%)]\tLoss: 0.063401\n",
      "Train Unlabeled Epoch: 4 [5120/47000 (11%)]\tLoss: 0.076722\n",
      "Train Unlabeled Epoch: 4 [7680/47000 (16%)]\tLoss: 0.075523\n",
      "Train Unlabeled Epoch: 4 [10240/47000 (22%)]\tLoss: 0.054135\n",
      "Train Unlabeled Epoch: 4 [12800/47000 (27%)]\tLoss: 0.081443\n",
      "Train Unlabeled Epoch: 4 [15360/47000 (33%)]\tLoss: 0.066073\n",
      "Train Unlabeled Epoch: 4 [17920/47000 (38%)]\tLoss: 0.071841\n",
      "Train Unlabeled Epoch: 4 [20480/47000 (43%)]\tLoss: 0.078368\n",
      "Train Unlabeled Epoch: 4 [23040/47000 (49%)]\tLoss: 0.052401\n",
      "Train Unlabeled Epoch: 4 [25600/47000 (54%)]\tLoss: 0.054540\n",
      "Train Unlabeled Epoch: 4 [28160/47000 (60%)]\tLoss: 0.078289\n",
      "Train Unlabeled Epoch: 4 [30720/47000 (65%)]\tLoss: 0.071972\n",
      "Train Unlabeled Epoch: 4 [33280/47000 (71%)]\tLoss: 0.058100\n",
      "Train Unlabeled Epoch: 4 [35840/47000 (76%)]\tLoss: 0.091740\n",
      "Train Unlabeled Epoch: 4 [38400/47000 (82%)]\tLoss: 0.076758\n",
      "Train Unlabeled Epoch: 4 [40960/47000 (87%)]\tLoss: 0.068598\n",
      "Train Unlabeled Epoch: 4 [43520/47000 (92%)]\tLoss: 0.089829\n",
      "Train Unlabeled Epoch: 4 [46080/47000 (98%)]\tLoss: 0.075334\n",
      "\n",
      "Test set: Average loss: 0.0882, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Labeled Epoch: 5 [0/3000 (0%)]\tLoss: 0.132647\n",
      "Train Labeled Epoch: 5 [320/3000 (11%)]\tLoss: 0.063633\n",
      "Train Labeled Epoch: 5 [640/3000 (21%)]\tLoss: 0.114526\n",
      "Train Labeled Epoch: 5 [960/3000 (32%)]\tLoss: 0.080563\n",
      "Train Labeled Epoch: 5 [1280/3000 (43%)]\tLoss: 0.082075\n",
      "Train Labeled Epoch: 5 [1600/3000 (53%)]\tLoss: 0.041916\n",
      "Train Labeled Epoch: 5 [1920/3000 (64%)]\tLoss: 0.044763\n",
      "Train Labeled Epoch: 5 [2240/3000 (74%)]\tLoss: 0.086986\n",
      "Train Labeled Epoch: 5 [2560/3000 (85%)]\tLoss: 0.026403\n",
      "Train Labeled Epoch: 5 [2880/3000 (96%)]\tLoss: 0.025202\n",
      "Train Unlabeled Epoch: 5 [0/47000 (0%)]\tLoss: 0.077852\n",
      "Train Unlabeled Epoch: 5 [2560/47000 (5%)]\tLoss: 0.062228\n",
      "Train Unlabeled Epoch: 5 [5120/47000 (11%)]\tLoss: 0.082663\n",
      "Train Unlabeled Epoch: 5 [7680/47000 (16%)]\tLoss: 0.075470\n",
      "Train Unlabeled Epoch: 5 [10240/47000 (22%)]\tLoss: 0.072090\n",
      "Train Unlabeled Epoch: 5 [12800/47000 (27%)]\tLoss: 0.078507\n",
      "Train Unlabeled Epoch: 5 [15360/47000 (33%)]\tLoss: 0.079648\n",
      "Train Unlabeled Epoch: 5 [17920/47000 (38%)]\tLoss: 0.069418\n",
      "Train Unlabeled Epoch: 5 [20480/47000 (43%)]\tLoss: 0.076615\n",
      "Train Unlabeled Epoch: 5 [23040/47000 (49%)]\tLoss: 0.061238\n",
      "Train Unlabeled Epoch: 5 [25600/47000 (54%)]\tLoss: 0.077758\n",
      "Train Unlabeled Epoch: 5 [28160/47000 (60%)]\tLoss: 0.065728\n",
      "Train Unlabeled Epoch: 5 [30720/47000 (65%)]\tLoss: 0.086373\n",
      "Train Unlabeled Epoch: 5 [33280/47000 (71%)]\tLoss: 0.075931\n",
      "Train Unlabeled Epoch: 5 [35840/47000 (76%)]\tLoss: 0.063951\n",
      "Train Unlabeled Epoch: 5 [38400/47000 (82%)]\tLoss: 0.059244\n",
      "Train Unlabeled Epoch: 5 [40960/47000 (87%)]\tLoss: 0.061234\n",
      "Train Unlabeled Epoch: 5 [43520/47000 (92%)]\tLoss: 0.059291\n",
      "Train Unlabeled Epoch: 5 [46080/47000 (98%)]\tLoss: 0.071399\n",
      "\n",
      "Test set: Average loss: 0.0874, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Labeled Epoch: 6 [0/3000 (0%)]\tLoss: 0.081083\n",
      "Train Labeled Epoch: 6 [320/3000 (11%)]\tLoss: 0.249643\n",
      "Train Labeled Epoch: 6 [640/3000 (21%)]\tLoss: 0.034771\n",
      "Train Labeled Epoch: 6 [960/3000 (32%)]\tLoss: 0.057943\n",
      "Train Labeled Epoch: 6 [1280/3000 (43%)]\tLoss: 0.073339\n",
      "Train Labeled Epoch: 6 [1600/3000 (53%)]\tLoss: 0.060032\n",
      "Train Labeled Epoch: 6 [1920/3000 (64%)]\tLoss: 0.086732\n",
      "Train Labeled Epoch: 6 [2240/3000 (74%)]\tLoss: 0.053927\n",
      "Train Labeled Epoch: 6 [2560/3000 (85%)]\tLoss: 0.148159\n",
      "Train Labeled Epoch: 6 [2880/3000 (96%)]\tLoss: 0.039665\n",
      "Train Unlabeled Epoch: 6 [0/47000 (0%)]\tLoss: 0.072192\n",
      "Train Unlabeled Epoch: 6 [2560/47000 (5%)]\tLoss: 0.074116\n",
      "Train Unlabeled Epoch: 6 [5120/47000 (11%)]\tLoss: 0.076988\n",
      "Train Unlabeled Epoch: 6 [7680/47000 (16%)]\tLoss: 0.066449\n",
      "Train Unlabeled Epoch: 6 [10240/47000 (22%)]\tLoss: 0.079462\n",
      "Train Unlabeled Epoch: 6 [12800/47000 (27%)]\tLoss: 0.062103\n",
      "Train Unlabeled Epoch: 6 [15360/47000 (33%)]\tLoss: 0.065304\n",
      "Train Unlabeled Epoch: 6 [17920/47000 (38%)]\tLoss: 0.079244\n",
      "Train Unlabeled Epoch: 6 [20480/47000 (43%)]\tLoss: 0.085357\n",
      "Train Unlabeled Epoch: 6 [23040/47000 (49%)]\tLoss: 0.077542\n",
      "Train Unlabeled Epoch: 6 [25600/47000 (54%)]\tLoss: 0.071688\n",
      "Train Unlabeled Epoch: 6 [28160/47000 (60%)]\tLoss: 0.071583\n",
      "Train Unlabeled Epoch: 6 [30720/47000 (65%)]\tLoss: 0.077294\n",
      "Train Unlabeled Epoch: 6 [33280/47000 (71%)]\tLoss: 0.084377\n",
      "Train Unlabeled Epoch: 6 [35840/47000 (76%)]\tLoss: 0.067083\n",
      "Train Unlabeled Epoch: 6 [38400/47000 (82%)]\tLoss: 0.066272\n",
      "Train Unlabeled Epoch: 6 [40960/47000 (87%)]\tLoss: 0.062690\n",
      "Train Unlabeled Epoch: 6 [43520/47000 (92%)]\tLoss: 0.093830\n",
      "Train Unlabeled Epoch: 6 [46080/47000 (98%)]\tLoss: 0.062816\n",
      "\n",
      "Test set: Average loss: 0.0875, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Labeled Epoch: 7 [0/3000 (0%)]\tLoss: 0.055553\n",
      "Train Labeled Epoch: 7 [320/3000 (11%)]\tLoss: 0.141378\n",
      "Train Labeled Epoch: 7 [640/3000 (21%)]\tLoss: 0.110006\n",
      "Train Labeled Epoch: 7 [960/3000 (32%)]\tLoss: 0.199437\n",
      "Train Labeled Epoch: 7 [1280/3000 (43%)]\tLoss: 0.058609\n",
      "Train Labeled Epoch: 7 [1600/3000 (53%)]\tLoss: 0.117050\n",
      "Train Labeled Epoch: 7 [1920/3000 (64%)]\tLoss: 0.066755\n",
      "Train Labeled Epoch: 7 [2240/3000 (74%)]\tLoss: 0.030747\n",
      "Train Labeled Epoch: 7 [2560/3000 (85%)]\tLoss: 0.066766\n",
      "Train Labeled Epoch: 7 [2880/3000 (96%)]\tLoss: 0.235791\n",
      "Train Unlabeled Epoch: 7 [0/47000 (0%)]\tLoss: 0.078972\n",
      "Train Unlabeled Epoch: 7 [2560/47000 (5%)]\tLoss: 0.061360\n",
      "Train Unlabeled Epoch: 7 [5120/47000 (11%)]\tLoss: 0.071522\n",
      "Train Unlabeled Epoch: 7 [7680/47000 (16%)]\tLoss: 0.063832\n",
      "Train Unlabeled Epoch: 7 [10240/47000 (22%)]\tLoss: 0.070117\n",
      "Train Unlabeled Epoch: 7 [12800/47000 (27%)]\tLoss: 0.066303\n",
      "Train Unlabeled Epoch: 7 [15360/47000 (33%)]\tLoss: 0.065129\n",
      "Train Unlabeled Epoch: 7 [17920/47000 (38%)]\tLoss: 0.056820\n",
      "Train Unlabeled Epoch: 7 [20480/47000 (43%)]\tLoss: 0.067856\n",
      "Train Unlabeled Epoch: 7 [23040/47000 (49%)]\tLoss: 0.072731\n",
      "Train Unlabeled Epoch: 7 [25600/47000 (54%)]\tLoss: 0.060607\n",
      "Train Unlabeled Epoch: 7 [28160/47000 (60%)]\tLoss: 0.081644\n",
      "Train Unlabeled Epoch: 7 [30720/47000 (65%)]\tLoss: 0.078759\n",
      "Train Unlabeled Epoch: 7 [33280/47000 (71%)]\tLoss: 0.067353\n",
      "Train Unlabeled Epoch: 7 [35840/47000 (76%)]\tLoss: 0.056744\n",
      "Train Unlabeled Epoch: 7 [38400/47000 (82%)]\tLoss: 0.063753\n",
      "Train Unlabeled Epoch: 7 [40960/47000 (87%)]\tLoss: 0.078346\n",
      "Train Unlabeled Epoch: 7 [43520/47000 (92%)]\tLoss: 0.043445\n",
      "Train Unlabeled Epoch: 7 [46080/47000 (98%)]\tLoss: 0.050963\n",
      "\n",
      "Test set: Average loss: 0.0874, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Labeled Epoch: 8 [0/3000 (0%)]\tLoss: 0.026527\n",
      "Train Labeled Epoch: 8 [320/3000 (11%)]\tLoss: 0.088055\n",
      "Train Labeled Epoch: 8 [640/3000 (21%)]\tLoss: 0.094948\n",
      "Train Labeled Epoch: 8 [960/3000 (32%)]\tLoss: 0.137767\n",
      "Train Labeled Epoch: 8 [1280/3000 (43%)]\tLoss: 0.087120\n",
      "Train Labeled Epoch: 8 [1600/3000 (53%)]\tLoss: 0.159569\n",
      "Train Labeled Epoch: 8 [1920/3000 (64%)]\tLoss: 0.037189\n",
      "Train Labeled Epoch: 8 [2240/3000 (74%)]\tLoss: 0.152034\n",
      "Train Labeled Epoch: 8 [2560/3000 (85%)]\tLoss: 0.025922\n",
      "Train Labeled Epoch: 8 [2880/3000 (96%)]\tLoss: 0.166999\n",
      "Train Unlabeled Epoch: 8 [0/47000 (0%)]\tLoss: 0.073499\n",
      "Train Unlabeled Epoch: 8 [2560/47000 (5%)]\tLoss: 0.065722\n",
      "Train Unlabeled Epoch: 8 [5120/47000 (11%)]\tLoss: 0.062543\n",
      "Train Unlabeled Epoch: 8 [7680/47000 (16%)]\tLoss: 0.093532\n",
      "Train Unlabeled Epoch: 8 [10240/47000 (22%)]\tLoss: 0.069687\n",
      "Train Unlabeled Epoch: 8 [12800/47000 (27%)]\tLoss: 0.055386\n",
      "Train Unlabeled Epoch: 8 [15360/47000 (33%)]\tLoss: 0.088535\n",
      "Train Unlabeled Epoch: 8 [17920/47000 (38%)]\tLoss: 0.042021\n",
      "Train Unlabeled Epoch: 8 [20480/47000 (43%)]\tLoss: 0.048406\n",
      "Train Unlabeled Epoch: 8 [23040/47000 (49%)]\tLoss: 0.058269\n",
      "Train Unlabeled Epoch: 8 [25600/47000 (54%)]\tLoss: 0.070421\n",
      "Train Unlabeled Epoch: 8 [28160/47000 (60%)]\tLoss: 0.082841\n",
      "Train Unlabeled Epoch: 8 [30720/47000 (65%)]\tLoss: 0.045072\n",
      "Train Unlabeled Epoch: 8 [33280/47000 (71%)]\tLoss: 0.077954\n",
      "Train Unlabeled Epoch: 8 [35840/47000 (76%)]\tLoss: 0.077581\n",
      "Train Unlabeled Epoch: 8 [38400/47000 (82%)]\tLoss: 0.086610\n",
      "Train Unlabeled Epoch: 8 [40960/47000 (87%)]\tLoss: 0.074549\n",
      "Train Unlabeled Epoch: 8 [43520/47000 (92%)]\tLoss: 0.068887\n",
      "Train Unlabeled Epoch: 8 [46080/47000 (98%)]\tLoss: 0.071524\n",
      "\n",
      "Test set: Average loss: 0.0876, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Labeled Epoch: 9 [0/3000 (0%)]\tLoss: 0.237337\n",
      "Train Labeled Epoch: 9 [320/3000 (11%)]\tLoss: 0.202637\n",
      "Train Labeled Epoch: 9 [640/3000 (21%)]\tLoss: 0.145098\n",
      "Train Labeled Epoch: 9 [960/3000 (32%)]\tLoss: 0.051153\n",
      "Train Labeled Epoch: 9 [1280/3000 (43%)]\tLoss: 0.068208\n",
      "Train Labeled Epoch: 9 [1600/3000 (53%)]\tLoss: 0.044853\n",
      "Train Labeled Epoch: 9 [1920/3000 (64%)]\tLoss: 0.104042\n",
      "Train Labeled Epoch: 9 [2240/3000 (74%)]\tLoss: 0.037084\n",
      "Train Labeled Epoch: 9 [2560/3000 (85%)]\tLoss: 0.035162\n",
      "Train Labeled Epoch: 9 [2880/3000 (96%)]\tLoss: 0.044184\n",
      "Train Unlabeled Epoch: 9 [0/47000 (0%)]\tLoss: 0.076042\n",
      "Train Unlabeled Epoch: 9 [2560/47000 (5%)]\tLoss: 0.056761\n",
      "Train Unlabeled Epoch: 9 [5120/47000 (11%)]\tLoss: 0.065875\n",
      "Train Unlabeled Epoch: 9 [7680/47000 (16%)]\tLoss: 0.060469\n",
      "Train Unlabeled Epoch: 9 [10240/47000 (22%)]\tLoss: 0.082123\n",
      "Train Unlabeled Epoch: 9 [12800/47000 (27%)]\tLoss: 0.073604\n",
      "Train Unlabeled Epoch: 9 [15360/47000 (33%)]\tLoss: 0.084697\n",
      "Train Unlabeled Epoch: 9 [17920/47000 (38%)]\tLoss: 0.066304\n",
      "Train Unlabeled Epoch: 9 [20480/47000 (43%)]\tLoss: 0.071362\n",
      "Train Unlabeled Epoch: 9 [23040/47000 (49%)]\tLoss: 0.083272\n",
      "Train Unlabeled Epoch: 9 [25600/47000 (54%)]\tLoss: 0.073867\n",
      "Train Unlabeled Epoch: 9 [28160/47000 (60%)]\tLoss: 0.073370\n",
      "Train Unlabeled Epoch: 9 [30720/47000 (65%)]\tLoss: 0.057969\n",
      "Train Unlabeled Epoch: 9 [33280/47000 (71%)]\tLoss: 0.081911\n",
      "Train Unlabeled Epoch: 9 [35840/47000 (76%)]\tLoss: 0.095382\n",
      "Train Unlabeled Epoch: 9 [38400/47000 (82%)]\tLoss: 0.054256\n",
      "Train Unlabeled Epoch: 9 [40960/47000 (87%)]\tLoss: 0.084491\n",
      "Train Unlabeled Epoch: 9 [43520/47000 (92%)]\tLoss: 0.070621\n",
      "Train Unlabeled Epoch: 9 [46080/47000 (98%)]\tLoss: 0.077527\n",
      "\n",
      "Test set: Average loss: 0.0878, Accuracy: 9729/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train_labeled(epoch)\n",
    "    train_unlabeled(epoch)\n",
    "\n",
    "    validate(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testset = pickle.load(open(\"../data/kaggle/test.p\", \"rb\"))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_predict = np.array([])\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = model(data)\n",
    "    temp = output.data.max(1)[1].numpy().reshape(-1)\n",
    "    \n",
    "    label_predict = np.concatenate((label_predict, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_predict\n",
    "\n",
    "predict_label = pd.DataFrame(label_predict, columns=['label'], dtype=int)\n",
    "predict_label.reset_index(inplace=True)\n",
    "predict_label.rename(columns={'index': 'ID'}, inplace=True)\n",
    "\n",
    "predict_label.head()\n",
    "\n",
    "predict_label.to_csv('../data/kaggle/sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
