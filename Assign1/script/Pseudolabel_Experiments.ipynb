{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                ])\n",
    "trainset_labeled_import = pickle.load(open(\"../data/kaggle/train_labeled.p\", \"rb\"))\n",
    "trainset_unlabeled_import = pickle.load(open(\"../data/kaggle/train_unlabeled.p\", \"rb\"))\n",
    "validset_import = pickle.load(open(\"../data/kaggle/validation.p\", \"rb\"))\n",
    "train_labeled_loader = torch.utils.data.DataLoader(trainset_labeled_import, batch_size=32, shuffle=True)\n",
    "train_unlabeled_loader = torch.utils.data.DataLoader(trainset_unlabeled_import, batch_size=256,\n",
    "                                                     shuffle=True)\n",
    "\n",
    "train_unlabeled_loader.dataset.train_labels = [-1 for i in range(len(train_unlabeled_loader.dataset.train_data))]\n",
    "valid_loader = torch.utils.data.DataLoader(validset_import, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.4230 -2.0857 -2.4230 -2.4230 -2.0699 -2.4230 -2.3342 -2.2204 -2.4225 -2.2946\n",
       "-2.3742 -2.1285 -2.3742 -2.3742 -2.1718 -2.2952 -2.3677 -2.3742 -2.3742 -2.2335\n",
       "-2.3802 -2.0872 -2.3802 -2.3778 -2.2749 -2.1184 -2.3277 -2.3802 -2.3802 -2.3802\n",
       "-2.3835 -2.0849 -2.3227 -2.3835 -2.2288 -2.2904 -2.3030 -2.3835 -2.3835 -2.3031\n",
       "-2.3603 -2.1602 -2.3546 -2.3603 -2.1501 -2.3465 -2.3603 -2.3174 -2.3603 -2.2879\n",
       "-2.4241 -1.9623 -2.3352 -2.4241 -2.3873 -2.2147 -2.2514 -2.2815 -2.4241 -2.4241\n",
       "-2.4228 -2.1905 -2.1599 -2.4228 -1.9354 -2.4228 -2.3395 -2.4228 -2.4228 -2.4228\n",
       "-2.3715 -2.3715 -2.3715 -2.1984 -1.9549 -2.3715 -2.3637 -2.3715 -2.3715 -2.3715\n",
       "-2.3742 -2.2783 -2.3742 -2.3090 -1.9508 -2.3742 -2.3282 -2.3742 -2.3742 -2.3742\n",
       "-2.4529 -2.0989 -2.3338 -2.4529 -2.2214 -2.1298 -2.3025 -2.4529 -2.4529 -2.2167\n",
       "-2.3855 -2.0893 -2.3855 -2.3855 -2.2527 -2.3422 -2.3047 -2.2136 -2.3855 -2.3255\n",
       "-2.4127 -2.3684 -2.4127 -2.4096 -2.0278 -2.4127 -1.9314 -2.4127 -2.4127 -2.3853\n",
       "-2.3540 -2.1958 -2.4034 -2.4034 -1.9802 -2.1783 -2.4034 -2.4034 -2.4034 -2.4034\n",
       "-2.3301 -2.2255 -2.3104 -2.3301 -2.1982 -2.3272 -2.3301 -2.3301 -2.3301 -2.3252\n",
       "-2.3914 -2.2005 -2.3914 -2.3327 -2.0005 -2.3914 -2.2530 -2.3914 -2.3608 -2.3914\n",
       "-2.3990 -2.3766 -2.3990 -2.3990 -2.0366 -2.3990 -2.0230 -2.3990 -2.3990 -2.3070\n",
       "-2.4359 -2.2112 -2.4359 -2.4359 -1.8450 -2.4359 -2.2284 -2.4359 -2.4359 -2.3069\n",
       "-2.3629 -2.0412 -2.3629 -2.3629 -2.3629 -2.3388 -2.2788 -2.2934 -2.3064 -2.3629\n",
       "-2.4091 -2.0666 -2.2852 -2.4091 -1.9494 -2.4091 -2.4091 -2.4091 -2.4091 -2.4091\n",
       "-2.3599 -2.0897 -2.3421 -2.3599 -2.1931 -2.3207 -2.3206 -2.3599 -2.3599 -2.3599\n",
       "-2.4295 -1.9548 -2.4295 -2.4295 -2.3116 -2.1571 -2.3576 -2.2728 -2.4295 -2.3685\n",
       "-2.3889 -2.1526 -2.3889 -2.3889 -2.0122 -2.3889 -2.3889 -2.3399 -2.2691 -2.3889\n",
       "-2.4485 -1.9393 -2.4485 -2.4485 -2.4057 -2.1071 -2.1430 -2.4485 -2.3542 -2.4485\n",
       "-2.4483 -2.2459 -2.3486 -2.4483 -1.8791 -2.3733 -2.3335 -2.2227 -2.4483 -2.4285\n",
       "-2.3647 -2.0698 -2.3647 -2.3647 -2.2937 -2.1620 -2.3647 -2.3647 -2.3647 -2.3647\n",
       "-2.2909 -2.2694 -2.1284 -2.3871 -2.1055 -2.3871 -2.3521 -2.3871 -2.3871 -2.3871\n",
       "-2.4671 -1.8734 -2.3451 -2.4671 -2.2038 -2.3729 -2.2874 -2.4671 -2.3633 -2.3345\n",
       "-2.4060 -2.0427 -2.4060 -2.4060 -2.4060 -2.2124 -2.2266 -2.1899 -2.4060 -2.4060\n",
       "-2.4168 -2.0531 -2.3449 -2.4168 -2.2925 -2.1215 -2.2540 -2.4168 -2.4168 -2.3738\n",
       "-2.4665 -1.8498 -2.4840 -2.4840 -2.2633 -2.1762 -2.4840 -2.3559 -2.3923 -2.2640\n",
       "-2.4323 -2.4323 -2.4323 -2.4323 -1.8436 -2.4323 -2.2551 -2.2725 -2.4323 -2.2367\n",
       "-2.3765 -2.2604 -2.3765 -2.3765 -2.2064 -2.2133 -2.1286 -2.3765 -2.3765 -2.3765\n",
       "[torch.FloatTensor of size 32x10]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d(0.3)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        #print(x.size())\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x)\n",
    "model = Net()\n",
    "a = torch.randn(32, 1, 28, 28)\n",
    "model.forward(Variable(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.3885 -2.3885 -2.3885 -2.3885 -2.2074 -2.1908 -2.3885 -2.3885 -2.3885 -1.9966\n",
       "-2.3157 -2.3157 -2.3013 -2.3157 -2.3157 -2.3157 -2.3157 -2.3157 -2.3157 -2.2044\n",
       "-2.3847 -2.3847 -2.3847 -2.3847 -2.3421 -2.3847 -2.3847 -2.0694 -2.2659 -2.1107\n",
       "-2.4014 -2.3823 -2.4014 -2.4014 -2.3507 -2.1476 -2.0121 -2.4014 -2.3287 -2.2820\n",
       "-2.3900 -2.3900 -2.3900 -2.2110 -2.3900 -2.2174 -2.3900 -2.3900 -1.9653 -2.3900\n",
       "-2.4278 -2.4278 -2.3567 -2.4278 -2.4278 -2.0843 -2.3407 -2.4278 -2.2954 -1.9469\n",
       "-2.3720 -2.3720 -2.3720 -2.3720 -2.3720 -2.2710 -2.2748 -2.0849 -2.3720 -2.2082\n",
       "-2.1829 -2.3394 -2.3394 -2.3394 -2.3076 -2.3394 -2.2784 -2.3394 -2.2605 -2.3115\n",
       "-2.3772 -2.3772 -2.3772 -2.2878 -2.3772 -2.1622 -2.3428 -2.2212 -2.3772 -2.1643\n",
       "-2.1357 -2.4849 -2.4849 -2.0544 -2.4335 -2.4849 -2.2952 -2.2487 -2.1664 -2.3530\n",
       "-2.4213 -2.4213 -2.4213 -2.4213 -2.0300 -2.4213 -2.4165 -2.4213 -1.9950 -2.2021\n",
       "-2.3539 -2.3599 -2.3599 -2.3599 -2.0902 -2.1211 -2.3599 -2.3599 -2.3556 -2.3599\n",
       "-2.4513 -2.4910 -2.1423 -2.0874 -2.2723 -2.3258 -2.4910 -2.4910 -2.4910 -1.9638\n",
       "-2.5551 -2.3564 -2.5551 -2.5551 -2.5551 -1.6685 -2.0153 -2.5551 -2.1427 -2.5551\n",
       "-2.2927 -2.3906 -2.3648 -2.3906 -2.0864 -2.3906 -2.3906 -2.3906 -2.0292 -2.3906\n",
       "-2.3179 -2.3179 -2.3179 -2.3179 -2.3179 -2.2613 -2.3179 -2.3179 -2.3179 -2.2262\n",
       "-2.3827 -2.3827 -2.3311 -2.2691 -2.1885 -2.3353 -2.3827 -2.3827 -2.0777 -2.3421\n",
       "-2.4495 -2.4495 -2.3570 -2.4495 -2.4495 -1.9995 -2.4062 -2.4495 -2.1200 -2.0557\n",
       "-2.3794 -2.3794 -2.3794 -2.3475 -2.3794 -2.2711 -2.3794 -2.1474 -2.3794 -2.0496\n",
       "-2.1975 -2.2616 -2.3044 -2.3586 -2.3593 -2.3817 -2.3817 -2.3817 -2.0691 -2.3817\n",
       "-2.3418 -2.3418 -2.3418 -2.3418 -2.3418 -2.3418 -2.3418 -2.3418 -2.3403 -2.0066\n",
       "-2.4289 -2.4289 -2.3316 -2.3377 -2.1046 -2.4063 -2.4289 -2.4289 -2.1582 -2.0698\n",
       "-2.3309 -2.3309 -2.3309 -2.3309 -2.2557 -2.2584 -2.3309 -2.3309 -2.3309 -2.2051\n",
       "-2.3746 -2.3746 -2.3746 -2.3746 -2.0428 -2.1736 -2.3546 -2.3746 -2.2697 -2.3746\n",
       "-2.4709 -2.4709 -2.4709 -2.1513 -2.3478 -2.4709 -2.3529 -2.3394 -2.1617 -1.9420\n",
       "-2.3561 -2.3561 -2.3561 -2.3561 -2.0001 -2.3561 -2.3561 -2.3561 -2.2407 -2.3561\n",
       "-2.4410 -2.4410 -2.4410 -2.4410 -1.8328 -2.2095 -2.2034 -2.3405 -2.4410 -2.4282\n",
       "-2.3158 -2.3730 -2.3730 -2.3730 -2.0007 -2.3730 -2.3730 -2.3730 -2.3703 -2.1768\n",
       "-2.4488 -2.4488 -2.4488 -2.2656 -2.1965 -2.3930 -2.0363 -2.4488 -2.0312 -2.4488\n",
       "-2.3595 -2.3595 -2.3595 -2.3595 -2.3117 -2.1590 -2.3595 -2.3595 -2.3595 -2.0859\n",
       "-2.4545 -2.4545 -2.4545 -2.0278 -2.4545 -2.0656 -2.4545 -2.4545 -2.4545 -1.9641\n",
       "-2.3979 -2.2771 -2.3979 -2.3979 -2.3979 -2.2969 -2.0704 -2.3737 -2.1910 -2.2815\n",
       "[torch.FloatTensor of size 32x10]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NetBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetBN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=0, padding=0, ceil_mode=True) # change\n",
    "\n",
    "        self.conv1_bn = nn.BatchNorm2d(64)\n",
    "        self.conv2_bn = nn.BatchNorm2d(128)\n",
    "        self.conv3_bn = nn.BatchNorm2d(256)\n",
    "        self.conv1_drop = nn.Dropout2d(0.2)\n",
    "        self.conv2_drop = nn.Dropout2d(0.3)\n",
    "        self.conv3_drop = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(32 * 8, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1_drop(F.relu(F.max_pool2d(self.conv1_bn(self.conv1(x)), (2,2))))\n",
    "\n",
    "        x = self.conv2_drop(F.relu(F.max_pool2d(self.conv2_bn(self.conv2(x)), (2,2))))\n",
    "\n",
    "        x = self.conv3_drop(F.relu(F.max_pool2d(self.conv3_bn(self.conv3(x)), (2,2))))\n",
    "        x = x.view(-1, 32 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = NetBN()\n",
    "a = torch.randn(32, 1, 28, 28)\n",
    "model.forward(Variable(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = NetBN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=.01, momentum=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_labeled_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "#         loss = my_criterion.forward(output, target, Variable(torch.LongTensor(epoch)), Variable(torch.LongTensor(1)))\n",
    "#         my_criterion.backward(loss)\n",
    "#         print(data.size())\n",
    "#         print(output.data.size(), target.data.size())\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Labeled Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_labeled_loader.dataset),\n",
    "                       100. * batch_idx / len(train_labeled_loader), loss.data[0]))\n",
    "\n",
    "#     for batch_idx, (data,_) in enumerate(train_unlabeled_loader):\n",
    "#         data = Variable(data)\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data)\n",
    "#         target = Variable(torch.LongTensor(output.data.max(1)[1].numpy().reshape(-1)))\n",
    "# #         loss = my_criterion.forward(output, target, Variable(torch.LongTensor(epoch)), Variable(torch.LongTensor(2)))\n",
    "# #         my_criterion.backward(loss)\n",
    "#         loss = F.nll_loss(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if batch_idx % 10 == 0:\n",
    "#             print('Train Unlabeled Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_unlabeled_loader.dataset),\n",
    "#                        100. * batch_idx / len(train_unlabeled_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(epoch, valid_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target)\n",
    "        \n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss.data.numpy()[0], correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labeled Epoch: 1 [0/3000 (0%)]\tLoss: 2.250507\n",
      "Train Labeled Epoch: 1 [320/3000 (11%)]\tLoss: 2.306233\n",
      "Train Labeled Epoch: 1 [640/3000 (21%)]\tLoss: 2.251914\n",
      "Train Labeled Epoch: 1 [960/3000 (32%)]\tLoss: 2.073837\n",
      "Train Labeled Epoch: 1 [1280/3000 (43%)]\tLoss: 1.868223\n",
      "Train Labeled Epoch: 1 [1600/3000 (53%)]\tLoss: 1.891294\n",
      "Train Labeled Epoch: 1 [1920/3000 (64%)]\tLoss: 1.425283\n",
      "Train Labeled Epoch: 1 [2240/3000 (74%)]\tLoss: 1.460097\n",
      "Train Labeled Epoch: 1 [2560/3000 (85%)]\tLoss: 1.130306\n",
      "Train Labeled Epoch: 1 [2880/3000 (96%)]\tLoss: 0.732923\n",
      "\n",
      "Test set: Average loss: 0.5256, Accuracy: 8822/10000 (88%)\n",
      "\n",
      "Train Labeled Epoch: 2 [0/3000 (0%)]\tLoss: 0.757906\n",
      "Train Labeled Epoch: 2 [320/3000 (11%)]\tLoss: 0.556363\n",
      "Train Labeled Epoch: 2 [640/3000 (21%)]\tLoss: 0.704225\n",
      "Train Labeled Epoch: 2 [960/3000 (32%)]\tLoss: 0.554556\n",
      "Train Labeled Epoch: 2 [1280/3000 (43%)]\tLoss: 0.683406\n",
      "Train Labeled Epoch: 2 [1600/3000 (53%)]\tLoss: 0.478517\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    validate(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testset = pickle.load(open(\"../data/kaggle/test.p\", \"rb\"))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCriterion(torch.autograd.Function):\n",
    "    def __init__(self):\n",
    "        self.alpha = .0005\n",
    "    \n",
    "    def forward(self, input, target, epoch, isLabeled):\n",
    "        loss = F.cross_entropy(input, target)\n",
    "\n",
    "        self.save_for_backward(input, target, epoch, isLabeled, loss)\n",
    "        print(self.saved_tensors)\n",
    "        if (isLabeled.data > 0).all():\n",
    "\n",
    "            return Variable(loss.data * self.alpha * epoch.data)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "       \n",
    "        input, target, epoch, isLabeled, loss, = self.saved_tensors\n",
    "        grad_input = loss.backward() \n",
    "        return grad_input\n",
    "my_criterion = MyCriterion()\n",
    "x = Variable(torch.randn(11, 10).type(torch.FloatTensor))\n",
    "y = Variable(torch.range(1,6, .5).type(torch.LongTensor))\n",
    "\n",
    "a = torch.from_numpy(np.array([0]))\n",
    "b = torch.from_numpy(np.array([1]))\n",
    "c = torch.from_numpy(np.array([10.0]))\n",
    "\n",
    "print(x)\n",
    "# print(torch.from_numpy(np.array([10])))\n",
    "first_loss  = my_criterion.forward(x, y, Variable(c.float()),  Variable(a))\n",
    "print(my_criterion.backward(first_loss))\n",
    "\n",
    "second_loss = my_criterion.forward(x, y, Variable(c.float()),  Variable(b))\n",
    "print(my_criterion.backward(second_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_predict = np.array([])\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = model(data)\n",
    "    temp = output.data.max(1)[1].numpy().reshape(-1)\n",
    "    \n",
    "    label_predict = np.concatenate((label_predict, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_predict\n",
    "\n",
    "predict_label = pd.DataFrame(label_predict, columns=['label'], dtype=int)\n",
    "predict_label.reset_index(inplace=True)\n",
    "predict_label.rename(columns={'index': 'ID'}, inplace=True)\n",
    "\n",
    "predict_label.head()\n",
    "\n",
    "predict_label.to_csv('../data/kaggle/sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
